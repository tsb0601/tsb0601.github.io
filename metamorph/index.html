<!doctype html>
<html lang="en">
<head>
    <title>MetaMorph: Multimodal Understanding and Generation via Instruction Tuning</title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" integrity="sha512-9usAa10IRO0HhonpyAIVpjrylPvoDwiPUiKdWk5t3PyolY1cOd4DSE0Ga+ri4AuTroPR5aQvXU9xC6qOPnzFeg==" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
        /* Base Styles */
        body {
            font-family: 'Inter', sans-serif; /* Using Inter font */
            margin: 0;
            padding: 0;
            background-color: var(--background);
            color: var(--text);
            line-height: 1.6;
        }

        /* Color Variables */
        :root {
            --primary: #2563eb;
            --secondary: #4f46e5;
            --accent: #7c3aed;
            --background: #f0f5ff; /* Light blue background */
            --text: #1e293b;
            --light-gray: #f8fafc;
            --medium-gray: #e5e7eb;
            --dark-gray: #6b7280;
        }

        /* Utility Classes */
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 2rem;
        }

        .section {
            padding: 4rem 0;
        }

        .section-title {
            text-align: center;
            margin-bottom: 3rem;
        }

        .section-title h2 {
            font-size: 2.5rem;
            background: linear-gradient(45deg, var(--primary), var(--accent));
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            margin-bottom: 0.5rem; /* Reduced margin */
        }

         .section-title p {
             color: var(--dark-gray);
             font-size: 1.1rem;
             max-width: 700px;
             margin: 0 auto; /* Center the paragraph */
         }

        .card {
            background: white;
            border-radius: 1rem;
            padding: 2rem;
            margin-bottom: 2rem;
            box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.05), 0 2px 4px -2px rgba(0, 0, 0, 0.05); /* Subtle shadow */
            transition: all 0.3s ease;
        }

        .card:hover {
             transform: translateY(-2px);
             box-shadow: 0 8px 12px -1px rgba(0, 0, 0, 0.08), 0 4px 6px -2px rgba(0, 0, 0, 0.05); /* Slightly larger shadow on hover */
        }

        .button {
            display: inline-flex;
            align-items: center;
            gap: 0.5rem;
            padding: 0.75rem 1.5rem;
            border-radius: 0.5rem;
            text-decoration: none;
            font-weight: 500;
            transition: all 0.2s;
            background: white;
            color: var(--text);
            border: 1px solid var(--medium-gray);
            cursor: pointer;
        }

        .button:hover {
            transform: translateY(-1px);
            box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1), 0 2px 4px -2px rgba(0, 0, 0, 0.06);
        }

        .button i { /* Style icons within buttons */
             font-size: 1.1em; /* Slightly larger icon */
        }

        .primary-button {
             background: linear-gradient(45deg, var(--primary), var(--secondary));
             color: white;
             border: none;
        }

        /* Header Styles */
        .header-wrapper {
            background: linear-gradient(135deg, var(--background), #f5f3ff);
            padding: 3rem 0; /* Adjusted padding */
            position: relative;
            overflow: hidden;
        }

        .header-wrapper::before {
            content: '';
            position: absolute;
            top: 0; left: 0; right: 0; bottom: 0;
            background: linear-gradient(45deg, rgba(37, 99, 235, 0.05), rgba(124, 58, 237, 0.05));
            z-index: 0;
        }

        .header-container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 2rem;
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 4rem;
            align-items: center;
            position: relative;
            z-index: 1;
        }

        .header-content h1 {
            font-size: 3.5rem;
            margin: 0 0 0.5rem 0; /* Added bottom margin */
            background: linear-gradient(45deg, var(--primary), var(--accent));
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            font-weight: 700; /* Bolder */
        }

        .header-content h2 {
            font-size: 1.8rem;
            font-weight: 500;
            color: var(--dark-gray); /* Use variable */
            margin: 0 0 1.5rem 0; /* Adjusted margin */
        }

         .header-content p { /* Style paragraph in header */
             font-size: 1.1rem;
             color: var(--text);
             margin-bottom: 1.5rem;
         }

        .emphasis-text {
            font-size: 1.4rem; /* Slightly smaller */
            font-weight: 500;
            margin: 2rem 0;
            padding: 1rem 1.5rem;
            background: linear-gradient(135deg, rgba(37, 99, 235, 0.1), rgba(124, 58, 237, 0.1));
            border-left: 4px solid var(--accent);
            border-radius: 0 8px 8px 0;
            color: var(--primary);
        }

        .emphasis-text strong {
            color: var(--accent);
            font-weight: 700; /* Bolder */
        }

        .header-image {
            position: relative;
        }

        .header-image img {
            width: 100%;
            border-radius: 1rem;
            box-shadow: 0 10px 25px -5px rgba(0, 0, 0, 0.1), 0 8px 10px -6px rgba(0, 0, 0, 0.1); /* Enhanced shadow */
        }

        .header-image::before { /* Decorative element */
            content: '';
            position: absolute;
            top: 1rem;
            right: -1rem;
            width: 100%;
            height: 100%;
            background: linear-gradient(45deg, var(--primary), var(--accent));
            border-radius: 1rem;
            z-index: -1;
            opacity: 0.1; /* More subtle */
        }

        .button-container {
            display: flex;
            flex-wrap: wrap;
            gap: 1rem;
            margin-top: 2rem;
        }

        /* Byline Styles */
        .byline {
            margin: 2rem 0;
            padding: 1.5rem 0; /* Adjusted padding */
            border-top: 1px solid var(--medium-gray);
            border-bottom: 1px solid var(--medium-gray);
        }

        .byline-container {
            max-width: 900px;
            margin: 0 auto;
            text-align: center;
            padding: 0 1rem; /* Add padding for smaller screens */
        }

        .authors {
            font-size: 1.1rem;
            line-height: 2;
            margin-bottom: 1rem;
        }

        .author-link {
            color: var(--text);
            text-decoration: none;
            transition: color 0.2s;
            white-space: nowrap;
        }

        .author-link:hover {
            color: var(--primary);
            text-decoration: underline; /* Add underline on hover */
        }

        .affiliations {
            font-size: 1rem;
            color: var(--dark-gray);
            margin: 1rem 0;
        }

        .affiliation-link {
            color: var(--dark-gray);
            text-decoration: none;
            transition: color 0.2s;
        }

        .affiliation-link:hover {
            color: var(--primary);
            text-decoration: underline; /* Add underline on hover */
        }

        .author-notes {
            font-size: 0.9rem;
            color: var(--dark-gray);
            margin-top: 1rem; /* Added margin */
        }

        .author-note {
            display: inline-block;
            margin: 0 0.5rem;
        }

        sup {
            font-size: 0.75em;
            line-height: 0;
            position: relative;
            vertical-align: baseline;
            top: -0.5em;
        }

        /* Abstract/Intro Section Styles */
        .abstract-section {
             padding: 3rem 0; /* Adjusted padding */
             background-color: white; /* White background for contrast */
        }

        .abstract-container {
            max-width: 900px; /* Slightly wider for readability */
            margin: 0 auto;
            padding: 0 2rem;
        }

        .intro-text {
            font-size: 1.15rem; /* Slightly larger */
            line-height: 1.8;
            color: var(--text);
            margin-bottom: 2rem; /* Spacing */
            text-align: justify; /* Justify text */
        }

        .intro-text strong {
            color: var(--primary);
            font-weight: 600; /* Medium bold */
        }

        .main-points {
            display: grid;
            gap: 1.5rem; /* Reduced gap */
            margin: 2rem 0; /* Adjusted margin */
        }

        .point-card {
            background: var(--light-gray); /* Light gray background */
            border-radius: 0.75rem; /* Slightly smaller radius */
            padding: 1.5rem;
            display: flex;
            gap: 1rem;
            align-items: flex-start;
            border: 1px solid var(--medium-gray); /* Subtle border */
        }

        .point-number {
            background: linear-gradient(45deg, var(--primary), var(--accent));
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            font-size: 1.8rem; /* Larger number */
            font-weight: bold;
            line-height: 1;
            min-width: 2rem; /* Ensure space for number */
            text-align: center;
        }

        .point-content {
            flex: 1;
            font-size: 1.05rem; /* Adjust font size */
        }

        .point-content strong {
            color: var(--primary);
            font-weight: 600;
        }

        /* News Feed Section */
        .news-section {
            background-color: white; /* Or var(--light-gray) */
            padding: 3rem 0;
            border-top: 1px solid var(--medium-gray);
        }

        .news-container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 2rem;
        }

        .news-item {
            background: var(--light-gray);
            border-radius: 0.75rem;
            padding: 1.5rem;
            margin-bottom: 1.5rem;
            border: 1px solid var(--medium-gray);
            border-left: 4px solid var(--primary); /* Accent border */
        }

        .news-date {
            font-size: 0.9rem;
            color: var(--dark-gray);
            margin-bottom: 0.5rem;
            font-weight: 500;
        }

        .news-title {
            font-size: 1.2rem;
            font-weight: 600;
            color: var(--text);
            margin-bottom: 0.5rem;
        }

        .news-content p {
            margin: 0;
            line-height: 1.6;
        }

         .news-content a {
            color: var(--primary);
            text-decoration: none;
            font-weight: 500;
         }

         .news-content a:hover {
             text-decoration: underline;
         }

        /* Demo Section Styles */
        .demo-section {
            background: linear-gradient(135deg, #f8faff 0%, #faf5ff 100%);
            padding: 4rem 0;
            border-top: 1px solid var(--medium-gray);
            border-bottom: 1px solid var(--medium-gray);
        }

        .demo-showcase {
            width: 80%;
            margin: 0 auto;
            background: white;
            border-radius: 1rem;
            overflow: hidden;
            box-shadow: 0 10px 25px -5px rgba(0, 0, 0, 0.1), 0 8px 10px -6px rgba(0, 0, 0, 0.1);
        }

        .demo-content {
            position: relative;
        }

        .demo-gif {
            width: 100%;
            height: auto;
            display: block;
        }

        .demo-description {
            padding: 1.5rem;
            text-align: center;
            border-top: 1px solid var(--medium-gray);
            background-color: var(--light-gray); /* Light background for description */
        }

        .demo-description h3 {
            color: var(--primary);
            margin: 0 0 0.5rem 0; /* Adjusted margin */
            font-size: 1.3rem;
        }

        .demo-description p {
            color: var(--dark-gray);
            line-height: 1.6;
            margin: 0;
        }

        /* Findings Section Styles */
        .findings, .vpit, .model-section { /* Shared section styles */
            background: linear-gradient(135deg, #f8faff 0%, #faf5ff 100%);
            padding: 4rem 0;
        }

        .findings-container, .vpit-container, .section-container { /* Shared container styles */
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 2rem;
        }

        .finding-card, .feature-card { /* Shared card styles */
             background: white;
             border-radius: 1rem;
             padding: 2rem;
             margin-bottom: 2rem;
             box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.05), 0 2px 4px -2px rgba(0, 0, 0, 0.05);
             transition: all 0.3s ease;
        }

        .finding-card:hover, .feature-card:hover {
             transform: translateY(-2px);
             box-shadow: 0 8px 12px -1px rgba(0, 0, 0, 0.08), 0 4px 6px -2px rgba(0, 0, 0, 0.05);
        }

        .finding-header, .feature-header { /* Shared header styles */
            display: flex;
            align-items: flex-start;
            justify-content: space-between;
            cursor: pointer;
            padding-bottom: 1rem; /* Add padding for spacing */
            border-bottom: 1px solid transparent; /* Prepare for border on expand */
            transition: border-color 0.3s ease;
        }

        .finding-header-content, .feature-header-content {
            flex: 1;
            padding-right: 1rem; /* Space before toggle button */
        }

        .finding-number {
            font-size: 1.2rem; /* Smaller */
            font-weight: bold;
            color: var(--accent);
            margin-bottom: 0.25rem; /* Reduced margin */
        }

        .finding-title, .feature-title {
            font-size: 1.4rem; /* Adjusted size */
            font-weight: 600; /* Medium bold */
            color: var(--text);
            margin-bottom: 0.25rem;
        }

        .finding-summary, .feature-summary {
            color: var(--dark-gray);
            font-size: 1.05rem; /* Adjusted size */
            line-height: 1.5;
        }

        .toggle-button {
            background: none;
            border: none;
            color: var(--primary);
            cursor: pointer;
            padding: 0.5rem;
            display: flex;
            align-items: center;
            justify-content: center;
            transition: transform 0.3s ease;
            font-size: 1.5rem; /* Make arrow bigger */
            line-height: 1;
        }

        .toggle-button svg { /* Keep SVG styling if used */
            width: 24px;
            height: 24px;
        }

        .finding-details, .feature-details {
            overflow: hidden;
            max-height: 0;
            opacity: 0;
            transition: max-height 0.5s ease, opacity 0.5s ease, padding-top 0.5s ease, margin-top 0.5s ease, border-top 0.5s ease; /* Smooth transitions */
            padding-top: 0;
            margin-top: 0;
            border-top: 1px solid transparent;
        }

        .finding-details.expanded, .feature-details.expanded {
            max-height: 3000px; /* Adjust as needed */
            opacity: 1;
            padding-top: 2rem;
            margin-top: 1.5rem; /* Add margin when expanded */
            border-top: 1px solid var(--medium-gray);
        }

        /* Specific styles for finding details */
        .finding-details {
             display: grid; /* Use grid for layout */
             grid-template-columns: 1fr; /* Default to single column */
             gap: 2rem;
        }

        @media (min-width: 768px) {
             .finding-details {
                 grid-template-columns: 1fr 1fr; /* Two columns on larger screens */
                 align-items: center; /* Vertically align items */
             }
        }

        .finding-visual {
            width: 100%; /* Full width */
            background: var(--light-gray);
            border-radius: 0.75rem;
            padding: 1rem;
            display: flex; /* Center image */
            justify-content: center;
            align-items: center;
            border: 1px solid var(--medium-gray);
        }

        .finding-visual img {
            max-width: 100%;
            height: auto;
            border-radius: 0.5rem;
            display: block; /* Remove extra space below image */
        }

        .finding-content {
            padding: 0; /* Remove extra padding */
        }

        .finding-description p {
            line-height: 1.7; /* Slightly more line height */
            font-size: 1.05rem;
            color: #374151;
            margin: 0 0 1rem 0; /* Consistent bottom margin */
        }
        .finding-description p:last-child {
            margin-bottom: 0;
        }

        /* VPiT Section Specifics */
        .method-diagram {
            background: white;
            padding: 2rem;
            border-radius: 1rem;
            box-shadow: 0 2px 4px rgba(0,0,0,0.05);
            margin-bottom: 2rem;
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); /* Responsive grid */
            gap: 2rem;
            align-items: stretch; /* Make steps same height */
        }

        .method-step {
            text-align: center;
            padding: 1.5rem;
            background: var(--light-gray);
            border-radius: 0.75rem;
            position: relative;
            border: 1px solid var(--medium-gray);
            display: flex; /* Use flexbox for alignment */
            flex-direction: column;
            justify-content: center; /* Center content vertically */
        }

        .method-step:not(:last-child)::after { /* Arrow connector */
            content: '→';
            position: absolute;
            right: -1.5rem;
            top: 50%;
            transform: translateY(-50%);
            color: var(--primary);
            font-size: 1.8rem; /* Larger arrow */
            display: none; /* Hide by default */
        }

        @media (min-width: 768px) { /* Show arrow on larger screens */
            .method-step:not(:last-child)::after {
                display: block;
            }
        }

        .method-title {
            font-weight: 600;
            color: var(--primary);
            margin-bottom: 0.5rem;
            font-size: 1.1rem; /* Adjust size */
        }

        .method-description {
            font-size: 0.95rem; /* Adjust size */
            color: var(--dark-gray);
        }

        /* VPiT Feature Card Layouts */
        #vpit-feature1.feature-details {
            display: grid;
            grid-template-columns: 1fr; /* Default single column */
            gap: 2rem;
        }
        @media (min-width: 992px) { /* Adjust breakpoint if needed */
             #vpit-feature1.feature-details {
                 grid-template-columns: 3fr 2fr; /* 60/40 split on large screens */
                 align-items: center;
             }
        }
        #vpit-feature1 .feature-content ul {
            padding-left: 1.5rem; /* Indent lists */
            margin-bottom: 1rem;
        }
        #vpit-feature1 .feature-content li {
             margin-bottom: 0.5rem;
        }
        #vpit-feature1 .feature-content ul ul {
             margin-top: 0.5rem;
             margin-bottom: 0.75rem;
        }
        #vpit-feature1 .feature-visual img {
            width: 100%;
            max-width: 400px; /* Limit max width */
            height: auto;
            display: block; /* Center image */
            margin: 0 auto;
            border-radius: 0.5rem;
        }

        #vpit-feature2.feature-details, #vpit-feature3.feature-details {
            display: flex;
            flex-direction: column;
            gap: 2rem;
        }
        #vpit-feature2 .feature-visual, #vpit-feature3 .feature-visual {
            width: 100%; /* Full width container */
            max-width: 800px; /* Max width for image */
            margin: 0 auto;
            background: var(--light-gray);
            border-radius: 0.75rem;
            padding: 1rem;
            border: 1px solid var(--medium-gray);
        }
        #vpit-feature2 .feature-visual img, #vpit-feature3 .feature-visual img {
            width: 100%;
            height: auto;
            border-radius: 0.5rem;
            display: block;
        }
         #vpit-feature2 .feature-content ul, #vpit-feature3 .feature-content ul {
             padding-left: 1.5rem;
             margin-bottom: 1rem;
         }
         #vpit-feature2 .feature-content li, #vpit-feature3 .feature-content li {
              margin-bottom: 0.5rem;
         }
         #vpit-feature2 .feature-content ul ul, #vpit-feature3 .feature-content ul ul {
              margin-top: 0.5rem;
              margin-bottom: 0.75rem;
         }

        /* MetaMorph Model Section Specifics */
        .table-container {
            overflow-x: auto;
            max-width: 100%;
            margin-bottom: 1rem;
            border: 1px solid var(--medium-gray); /* Add border */
            border-radius: 0.5rem; /* Rounded corners */
        }

        .data-table {
            border-collapse: collapse;
            width: 100%;
            min-width: 800px; /* Ensure minimum width for scrolling */
            font-size: 0.9rem;
        }

        .data-table th, .data-table td {
            padding: 0.75rem; /* Increased padding */
            text-align: center;
            border: 1px solid var(--medium-gray);
            white-space: nowrap; /* Prevent wrapping */
        }

        .data-table th {
            background-color: var(--light-gray);
            font-weight: 600;
            position: sticky; /* Make header sticky */
            top: 0;
            z-index: 1; /* Ensure header is above content */
        }

        .tb-hdr { /* Main header row */
            background-color: #e0e7ff; /* Light indigo */
            font-weight: 700;
        }

        .section-border {
            border-right: 2px solid #adb5bd; /* Darker border */
        }

        .highlight-gray td {
            background-color: #f1f5f9; /* Slightly darker gray */
            font-style: italic;
            font-weight: 500;
        }

        .highlight-orange td {
            background-color: #fff7ed;
            font-weight: 600;
        }

        td .highlight { /* Highlight specific cells if needed */
            font-weight: bold;
            color: var(--primary);
        }

        .performance-details.expanded {
             max-height: none; /* Allow table to expand fully */
        }

        .performance-text {
             margin-top: 1.5rem; /* Space above text */
             color: #374151;
             line-height: 1.7;
             font-size: 1.05rem;
        }
        .performance-text p:last-of-type {
            margin-bottom: 0;
        }

        #tab\:final_table figcaption { /* Style table caption */
             text-align: center;
             margin-top: 1rem;
             font-size: 0.9rem;
             color: var(--dark-gray);
             padding: 0 1rem; /* Add padding */
             width: auto; /* Remove fixed width */
        }

        /* Example Sliders */
        .examples-container {
            width: 100%;
            padding: 1rem;
            background: var(--light-gray); /* Background for slider area */
            border-radius: 0.75rem;
            margin-top: 1rem; /* Space above slider */
            border: 1px solid var(--medium-gray);
        }

        .example-nav {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 1rem;
            padding: 0 0.5rem;
        }

        .example-nav .nav-button { /* Style slider buttons */
            background: white;
            border: 1px solid var(--medium-gray);
            cursor: pointer;
            font-size: 1.25rem;
            color: var(--primary);
            padding: 0.5rem 0.8rem; /* Adjust padding */
            border-radius: 0.5rem;
            transition: all 0.2s ease;
            line-height: 1; /* Ensure vertical alignment */
        }

        .example-nav .nav-button:hover {
            background: var(--primary);
            color: white;
            transform: translateY(-1px);
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
        }

        #example-counter, #reasoning-counter {
            font-size: 0.95rem;
            color: var(--dark-gray);
            font-weight: 500;
            background: white;
            padding: 0.4rem 0.8rem;
            border-radius: 0.5rem;
            border: 1px solid var(--medium-gray);
        }

        .example, .reasoning-example {
            display: none; /* Hide inactive slides */
            opacity: 0;
            transition: opacity 0.4s ease-in-out;
            width: 100%;
        }

        .example.active, .reasoning-example.active {
            display: block; /* Show active slide */
            opacity: 1;
        }

        /* Knowledge Leverage Example Styles */
        .example img {
            width: 100%;
            max-width: 400px; /* Limit image size */
            height: auto;
            border-radius: 0.75rem;
            margin: 0 auto 1rem auto; /* Center image */
            display: block;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        }
        .example-text {
            background: white;
            padding: 1.5rem;
            border-radius: 0.75rem;
            border: 1px solid var(--medium-gray);
        }
        .prompt, .analysis { margin-bottom: 1rem; }
        .prompt:last-child, .analysis:last-child { margin-bottom: 0; }
        .prompt h4, .analysis h4 {
            font-weight: 600;
            margin-bottom: 0.5rem;
            color: var(--primary);
            font-size: 1.05rem;
        }
        .prompt p, .analysis p {
            color: #4b5563;
            line-height: 1.6;
            font-size: 1rem;
            margin: 0;
        }

        /* Reasoning Example Styles */
        .reasoning-example {
             display: none; /* Hide by default */
             opacity: 0;
             transition: opacity 0.4s ease-in-out;
             width: 100%;
             text-align: center; /* Center content */
        }
        .reasoning-example.active {
             display: block;
             opacity: 1;
        }
        .prompt-box {
            background: linear-gradient(135deg, var(--primary), var(--accent));
            padding: 1rem 1.5rem;
            border-radius: 0.75rem;
            color: white;
            font-size: 1.1rem;
            line-height: 1.5;
            margin: 0 auto 1.5rem auto; /* Center and add bottom margin */
            box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);
            max-width: 90%; /* Limit width */
            text-align: left; /* Align text left */
        }
        .result-image {
            margin: 1.5rem auto;
            padding: 0.5rem;
            max-width: 300px; /* Control image size */
            width: 80%; /* Responsive width */
            display: inline-block; /* Allows centering via text-align */
            background: white;
            border-radius: 0.75rem;
            box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);
        }
        .result-image img {
            width: 100%;
            height: auto;
            border-radius: 0.5rem;
            display: block;
        }
        .reasoning-process {
            background: white; /* White background */
            padding: 1.5rem;
            border-radius: 1rem;
            margin-top: 1.5rem;
            border: 1px solid var(--medium-gray);
            text-align: left; /* Align text left */
        }
        .process-header {
            text-align: center;
            color: var(--primary);
            font-weight: 600;
            margin-bottom: 1.5rem;
            font-size: 1.1rem;
        }
        .steps-progress {
            display: flex;
            align-items: center;
            justify-content: center;
            margin-bottom: 1.5rem;
        }
        .step-dot {
            width: 32px; height: 32px; border-radius: 50%;
            background: white; border: 2px solid var(--medium-gray);
            display: flex; align-items: center; justify-content: center;
            font-weight: 600; color: var(--dark-gray);
            cursor: pointer; transition: all 0.3s ease;
        }
        .step-dot:hover { transform: scale(1.1); }
        .step-dot.active { background: var(--primary); border-color: var(--primary); color: white; }
        .step-line { height: 2px; width: 60px; background: var(--medium-gray); margin: 0 8px; }
        .steps-content { position: relative; min-height: 80px; /* Ensure space for content */ }
        .step-box {
            background: var(--light-gray); padding: 1rem; border-radius: 0.75rem;
            border: 1px solid var(--medium-gray);
            position: absolute; width: 100%; left:0; top: 0; /* Position for fade effect */
            transition: opacity 0.3s ease, transform 0.3s ease;
            opacity: 1; transform: translateY(0);
        }
        .step-box.hidden { opacity: 0; transform: translateY(10px); pointer-events: none; }
        .step-title { color: var(--primary); font-weight: 600; margin-bottom: 0.5rem; }
        .step-detail { color: #4b5563; line-height: 1.5; font-size: 0.95rem; }

        /* Responsive Adjustments */
        @media (max-width: 992px) {
            .header-container { grid-template-columns: 1fr; gap: 2rem; text-align: center; }
            .header-image { grid-row: 1; margin-bottom: 2rem; }
            .header-image::before { display: none; } /* Hide decorative element on mobile */
            .button-container { justify-content: center; }
            .demo-showcase { width: 95%; }
            .finding-details { grid-template-columns: 1fr; } /* Stack finding details */
            .method-diagram { grid-template-columns: 1fr; } /* Stack method steps */
            .method-step:not(:last-child)::after { display: none; } /* Hide arrows */
        }

        @media (max-width: 768px) {
            .header-content h1 { font-size: 2.8rem; }
            .header-content h2 { font-size: 1.5rem; }
            .emphasis-text { font-size: 1.2rem; }
            .section-title h2 { font-size: 2rem; }
            .authors { font-size: 1rem; line-height: 1.8; }
            .point-card { flex-direction: column; align-items: center; text-align: center; } /* Stack point card content */
            .point-number { margin-bottom: 0.5rem; }
            .vpit-container .feature-details, .model-section .feature-details { grid-template-columns: 1fr; } /* Stack feature details */
            #vpit-feature1.feature-details { grid-template-columns: 1fr; } /* Ensure single column */
            #vpit-feature1 .feature-visual img { max-width: 300px; }
            .result-image { max-width: 200px; } /* Smaller reasoning image */
            .prompt-box { font-size: 1rem; }
            .step-line { width: 40px; }
        }

        @media (max-width: 480px) {
            .container, .abstract-container, .news-container, .findings-container, .vpit-container, .section-container { padding: 0 1rem; }
            .header-content h1 { font-size: 2.2rem; }
            .header-content h2 { font-size: 1.3rem; }
            .button { padding: 0.6rem 1.2rem; font-size: 0.9rem; }
            .card, .point-card, .finding-card, .feature-card { padding: 1.5rem; }
            .section-title h2 { font-size: 1.8rem; }
            .finding-title, .feature-title { font-size: 1.2rem; }
            .finding-summary, .feature-summary { font-size: 1rem; }
            .prompt-box { font-size: 0.95rem; padding: 0.8rem 1.2rem; }
        }

        /* Add Inter font */
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap');

    </style>
</head>
<body>
    <div class="header-wrapper">
        <div class="header-container">
            <div class="header-content">
                <h1>MetaMorph</h1>
                <h2>Multimodal Understanding and Generation via Instruction Tuning</h2>
                <p>
                    We demonstrate that Large Language Models (LLMs) can be effectively finetuned into unified multimodal models capable of both understanding and generation using instruction tuning.
                </p>

                <div class="emphasis-text">
                    LLMs are already <strong>VERY CLOSE</strong> to being Unified Multimodal Models!
                </div>

                <div class="button-container">
                    <a href="https://arxiv.org/abs/2412.14164v1" class="button primary-button" target="_blank" rel="noopener noreferrer">
                        <i class="fas fa-file-alt"></i> arXiv Paper
                    </a>
                    <a href="https://github.com/facebookresearch/metamorph/" class="button" target="_blank" rel="noopener noreferrer">
                        <i class="fab fa-github"></i> GitHub Code
                    </a>
                </div>
            </div>
            <div class="header-image">
                <img src="static/img/metamorph_teaser.webp"
                     alt="MetaMorph Overview Teaser"
                     class="teaser-image"
                     onerror="this.onerror=null; this.src='https://placehold.co/600x400/e0e7ff/1e293b?text=MetaMorph+Teaser';">
            </div>
        </div>
    </div>

    <div class="byline">
        <div class="byline-container">
            <p class="authors">
                <a href="https://tsb0601.github.io/petertongsb/" class="author-link" target="_blank">Shengbang Tong<sup>1,2,*,†</sup></a> &emsp;
                <a href="https://davidfan.io/" class="author-link" target="_blank">David Fan<sup>1</sup></a> &emsp;
                <a href="https://jiachenzhu.github.io/" class="author-link" target="_blank">Jiachen Zhu<sup>1,2,*</sup></a> &emsp;
                <a href="https://pages.cs.wisc.edu/~yunyang/" class="author-link" target="_blank">Yunyang Xiong<sup>3</sup></a> &emsp;
                <a href="https://xinleic.xyz/" class="author-link" target="_blank">Xinlei Chen<sup>1</sup></a> &emsp;
                <br>
                <a href="https://koustuvsinha.com/" class="author-link" target="_blank">Koustuv Sinha<sup>1</sup></a> &emsp;
                <a href="https://ai.meta.com/people/1148536089838617/michael-rabbat/" class="author-link" target="_blank">Michael Rabbat<sup>1</sup></a> &emsp;
                <a href="https://yann.lecun.com/" class="author-link" target="_blank">Yann LeCun<sup>1,2</sup></a> &emsp;
                <a href="https://www.sainingxie.com/" class="author-link" target="_blank">Saining Xie<sup>2</sup></a> &emsp;
                <a href="https://liuzhuang13.github.io/" class="author-link" target="_blank">Zhuang Liu<sup>1,†</sup></a>
            </p>
            <div class="affiliations">
                <p>
                    <a href="https://fair.meta.com/" class="affiliation-link" target="_blank"><sup>1</sup>FAIR, Meta</a> &emsp;
                    <a href="https://cs.nyu.edu/home/index.html" class="affiliation-link" target="_blank"><sup>2</sup>New York University</a> &emsp;
                    <a href="#" class="affiliation-link" target="_blank"><sup>3</sup>Meta Reality Labs</a>
                </p>
            </div>
            <p class="author-notes">
                <span class="author-note"><sup>*</sup>Work done at Meta</span> &emsp;
                <span class="author-note"><sup>†</sup>Corresponding authors</span>
            </p>
        </div>
    </div>


    
    <section class="news-section section">
        <div class="news-container">
            <div class="section-title">
                <h2>News & Updates</h2>
            </div>
            <div class="news-item">
                <div class="news-date">April 14, 2025</div>
                <div class="news-title">Training Code Released!</div>
                <div class="news-content">
                    <p>We release of the MetaMorph training code. You can find it on our <a href="https://github.com/facebookresearch/metamorph/" target="_blank" rel="noopener noreferrer">GitHub repository</a>. Explore the code and experiment with Visual-Predictive Instruction Tuning!</p>
                </div>
            </div>
            </div>
    </section>
    

    <section class="abstract-section section">
        <div class="abstract-container">
             <div class="section-title">
                 <h2>Abstract</h2>
             </div>
            <div class="intro-text">
                <p>
                    We extend Visual Instruction Tuning to <strong>Visual-Predictive Instruction Tuning (VPiT)</strong> to study unified multimodal models. This simple yet effective approach enables LLMs to predict both visual and text tokens through instruction tuning, without requiring extensive architectural changes or pretraining.
                </p>
            </div>

            <div class="main-points">
                <div class="point-card">
                    <div class="point-number">1</div>
                    <div class="point-content">
                        We discover that <strong>generation and understanding are mutually beneficial</strong>. Through extensive experiments, we reveal that visual generation emerges naturally as models improve at understanding—requiring as little as 200K samples when co-trained, compared to millions needed traditionally.
                    </div>
                </div>

                <div class="point-card">
                    <div class="point-number">2</div>
                    <div class="point-content">
                        Our <strong>Visual-Predictive Instruction Tuning (VPiT)</strong> extends existing instruction tuning to predict continuous visual tokens alongside discrete text tokens. This simple modification unlocks powerful multimodal capabilities while maintaining the efficiency of instruction tuning.
                    </div>
                </div>

                <div class="point-card">
                    <div class="point-number">3</div>
                    <div class="point-content">
                        We train the <strong>MetaMorph</strong> model using VPiT, achieving competitive performance across benchmarks. More importantly, we find intriguing evidence of modality unification: the model can leverage LLM knowledge for generation and perform implicit reasoning before generating visual tokens.
                    </div>
                </div>
            </div>
        </div>
    </section>



    <section class="demo-section section">
        <div class="container"> <div class="section-title">
                <h2>MetaMorph Examples</h2>
                <p>See our unified multimodal model performing various tasks, including understanding, generation, and implicit reasoning.</p>
            </div>

            <div class="demo-showcase">
                <div class="demo-content">
                    <img src="static/img/examples.gif" alt="MetaMorph Demo GIF" class="demo-gif"
                         onerror="this.onerror=null; this.src='https://placehold.co/800x450/e0e7ff/1e293b?text=MetaMorph+Examples';">
                </div>
                <div class="demo-description">
                    <h3>Multimodal Understanding & Generation</h3>
                    <p>Watch as MetaMorph handles both visual understanding and generation tasks after instruction tuning.</p>
                </div>
            </div>
        </div>
    </section>

    <section class="findings section" id="findings">
        <div class="findings-container">
            <div class="section-title">
                <h2>Key Findings</h2>
                <p>Visual Understanding and Visual Generation are Coupled!</p>
            </div>

            <div class="finding-card card">
                <div class="finding-header" onclick="toggleDetails('finding1')">
                    <div class="finding-header-content">
                        <div class="finding-number">Finding 1</div>
                        <div class="finding-title">Visual Generation Emerges Naturally From Understanding</div>
                        <div class="finding-summary">Only a small number of samples are needed to unlock visual generation when co-training with understanding tasks.</div>
                    </div>
                    <button class="toggle-button" id="toggle1">↓</button>
                </div>
                <div class="finding-details" id="finding1">
                    <div class="finding-visual">
                        <img src="static/img/fid_special_points_plot_new.jpg" alt="Data Efficiency Comparison Graph"
                             onerror="this.onerror=null; this.src='https://placehold.co/500x300/f8fafc/6b7280?text=Finding+1+Graph';">
                    </div>
                    <div class="finding-content">
                        <div class="finding-description">
                            <p>Visual generation capability can be unlocked with significantly less data when co-trained with visual understanding tasks. Our experiments show that 5,000 examples are enough to trigger visual generation, and 200K samples are sufficient to generate high-quality visual tokens when combined with understanding tasks, compared to millions needed for pure generation training.</p>
                        </div>
                    </div>
                </div>
            </div>

            <div class="finding-card card">
                <div class="finding-header" onclick="toggleDetails('finding2')">
                    <div class="finding-header-content">
                        <div class="finding-number">Finding 2</div>
                        <div class="finding-title">Visual Understanding and Generation are Synergistic</div>
                        <div class="finding-summary">Better understanding leads to better generation, and vice versa.</div>
                    </div>
                    <button class="toggle-button" id="toggle2">↓</button>
                </div>
                <div class="finding-details" id="finding2">
                    <div class="finding-visual">
                         <img src="static/img/understanding_generation.PNG" alt="Understanding-Generation Correlation Graph"
                              onerror="this.onerror=null; this.src='https://placehold.co/500x300/f8fafc/6b7280?text=Finding+2+Graph';">
                    </div>
                    <div class="finding-content">
                        <div class="finding-description">
                            <p>There exists a strong correlation between visual understanding and generation capabilities. As models improve their understanding abilities (e.g., VQA scores), their generation capabilities (e.g., lower FID scores) naturally enhance, and vice versa, creating a powerful synergistic effect.</p>
                        </div>
                    </div>
                </div>
            </div>

             <div class="finding-card card">
                 <div class="finding-header" onclick="toggleDetails('finding3')">
                     <div class="finding-header-content">
                         <div class="finding-number">Finding 3</div>
                         <div class="finding-title">Understanding Data Boosts Both Capabilities More Effectively</div>
                         <div class="finding-summary">Understanding data significantly improves both understanding and generation performance compared to generation data alone.</div>
                     </div>
                     <button class="toggle-button" id="toggle3">↓</button>
                 </div>
                 <div class="finding-details" id="finding3">
                     <div class="finding-visual">
                          <img src="static/img/heatmap.PNG" alt="Data Impact Comparison Heatmap"
                               onerror="this.onerror=null; this.src='https://placehold.co/500x300/f8fafc/6b7280?text=Finding+3+Heatmap';">
                     </div>
                     <div class="finding-content">
                         <div class="finding-description">
                             <p>The darker the heatmap cell, the better the performance. Understanding data proves to be significantly more valuable than generation data for improving both understanding (e.g., MMBench) and generation (e.g., FID) tasks. For example, with the same total data amount (5M samples), using 4M VQA + 1M Generation data yields better results on both task types than using 1M VQA + 4M Generation data.</p>
                         </div>
                     </div>
                 </div>
             </div>

             <div class="finding-card card">
                 <div class="finding-header" onclick="toggleDetails('finding4')">
                     <div class="finding-header-content">
                         <div class="finding-number">Finding 4</div>
                         <div class="finding-title">Visual Generation Aligns With Visually Demanding Tasks</div>
                         <div class="finding-summary">Generation ability strongly correlates with general, text & chart, and vision-centric tasks, but less so with knowledge-based tasks.</div>
                     </div>
                     <button class="toggle-button" id="toggle4">↓</button>
                 </div>
                 <div class="finding-details" id="finding4">
                     <div class="finding-visual">
                          <img src="static/img/benchmark_analysis.PNG" alt="Task Correlation Analysis Graph"
                               onerror="this.onerror=null; this.src='https://placehold.co/500x300/f8fafc/6b7280?text=Finding+4+Graph';">
                     </div>
                     <div class="finding-content">
                         <div class="finding-description">
                             <p>Visual generation capabilities (measured by FID) show strong correlation with visually demanding understanding tasks (like MMBench-General, MMBench-V&C, MMBench-VisionCentric) but weaker correlation with knowledge-based tasks (MMBench-Knowledge). This suggests that visual understanding and generation are fundamental visual capabilities intertwined within autoregressive models.</p>
                         </div>
                     </div>
                 </div>
             </div>
        </div>

        <script>
            // Simple toggle script for findings sections
            function toggleDetails(id) {
                const detailsElement = document.getElementById(id);
                const buttonElement = document.getElementById('toggle' + id.slice(-1)); // Assumes id ends with a number
                const isExpanded = detailsElement.classList.contains('expanded');

                // Optional: Collapse others if you want an accordion effect
                // document.querySelectorAll('.finding-details.expanded').forEach(el => {
                //     if (el.id !== id) {
                //         el.classList.remove('expanded');
                //         const otherButtonId = 'toggle' + el.id.slice(-1);
                //         document.getElementById(otherButtonId).style.transform = 'rotate(0deg)';
                //         document.getElementById(otherButtonId).innerHTML = '↓';
                //     }
                // });

                detailsElement.classList.toggle('expanded');
                if (buttonElement) {
                     buttonElement.style.transform = isExpanded ? 'rotate(0deg)' : 'rotate(180deg)';
                     buttonElement.innerHTML = isExpanded ? '↓' : '↑'; // Change arrow direction
                }
            }
        </script>
    </section>

    <section class="vpit section" id="vpit">
        <div class="vpit-container">
            <div class="section-title">
                <h2>Visual Predictive Instruction Tuning (VPiT)</h2>
                <p>A simple yet effective extension enabling LLMs to predict both visual and text tokens.</p>
            </div>

            <div class="method-diagram">
                <div class="method-step">
                    <div class="method-title">Multimodal Input</div>
                    <div class="method-description">Process visual (image/video frames) and text tokens in any sequence order.</div>
                </div>
                <div class="method-step">
                    <div class="method-title">Unified Processing</div>
                    <div class="method-description">LLM backbone with separate, lightweight text and vision prediction heads.</div>
                </div>
                <div class="method-step">
                    <div class="method-title">Token Generation</div>
                    <div class="method-description">Generate text tokens (via softmax) and continuous visual tokens (via projection). Visualize visual tokens using a diffusion model.</div>
                </div>
            </div>

            <div class="feature-card card" onclick="toggleVPiTFeature('vpit-feature1')">
                 <div class="feature-header">
                     <div class="feature-header-content">
                         <div class="feature-title">Training Process</div>
                         <div class="feature-summary">Multimodal next-token prediction with instruction tuning.</div>
                     </div>
                     <button class="toggle-button" id="toggle-vpit1">↓</button>
                 </div>
                 <div class="feature-details" id="vpit-feature1">
                     <div class="feature-content">
                         <p>VPiT extends visual instruction tuning with:</p>
                         <ul>
                             <li><strong>Multimodal Input Processing:</strong>
                                 <ul>
                                     <li>Visual inputs (images/frames) processed through a pretrained vision encoder (e.g., SigLIP).</li>
                                     <li>Features are interpolated to a fixed number of visual tokens (e.g., 64).</li>
                                     <li>A trainable projection layer matches visual token dimensions to the LLM's embedding space.</li>
                                 </ul>
                             </li>
                             <li><strong>Model Architecture:</strong>
                                 <ul>
                                     <li>Uses a standard LLM backbone (e.g., LLaMA).</li>
                                     <li>Adds two lightweight prediction heads on top of the LLM's output embeddings:</li>
                                     <li><em>Language Head:</em> Standard linear layer for text token prediction (trained with cross-entropy loss).</li>
                                     <li><em>Vision Head:</em> MLP projecting LLM embeddings back to the vision encoder's feature dimension (trained with cosine similarity loss against ground truth visual tokens).</li>
                                     <li>Only the LLM, adapter layers, and prediction heads are trained.</li>
                                 </ul>
                             </li>
                             <li><strong>Token Prediction:</strong>
                                 <ul>
                                     <li>Predicts the next token autoregressively, whether text or visual.</li>
                                     <li>Uses cross-entropy loss for text tokens via the language head.</li>
                                     <li>Uses cosine similarity loss for visual tokens via the vision head.</li>
                                     <li>Special tokens <code>&lt;image_start&gt;</code> and <code>&lt;image_end&gt;</code> delineate visual token sequences.</li>
                                     <li>Loss is only calculated for response tokens (masked inputs).</li>
                                 </ul>
                             </li>
                             <li><strong>Unified Framework:</strong>
                                 <ul>
                                     <li>Maintains the efficiency of instruction tuning.</li>
                                     <li>Handles interleaved text and visual inputs/outputs naturally.</li>
                                     <li>Requires minimal architectural changes to existing LLMs.</li>
                                 </ul>
                             </li>
                         </ul>
                     </div>
                     <div class="feature-visual">
                         <img src="static/img/VPT.png" alt="VPiT Training Process Diagram"
                              onerror="this.onerror=null; this.src='https://placehold.co/400x300/f8fafc/6b7280?text=VPiT+Diagram';">
                     </div>
                 </div>
             </div>

             <div class="feature-card card" onclick="toggleVPiTFeature('vpit-feature2')">
                 <div class="feature-header">
                     <div class="feature-header-content">
                         <div class="feature-title">Training Data Types</div>
                         <div class="feature-summary">Utilizes a broad range of multimodal data formatted as instructions.</div>
                     </div>
                     <button class="toggle-button" id="toggle-vpit2">↓</button>
                 </div>
                 <div class="feature-details" id="vpit-feature2">
                     <div class="feature-visual">
                         <img src="static/img/data.png" alt="Data Categories Diagram"
                              onerror="this.onerror=null; this.src='https://placehold.co/800x200/f8fafc/6b7280?text=Data+Categories';">
                     </div>
                     <div class="feature-content">
                         <p>Three major data categories are formatted into instruction-tuning pairs:</p>
                         <ul>
                             <li><strong>Visual Understanding Data:</strong>
                                 <ul>
                                     <li>ImageQA: Utilizes the Cambrian-7M collection (LLaVA, ShareGPT4V, etc.).</li>
                                     <li>VideoQA: Includes VideoStar and ShareVideo datasets (processed at 1 FPS).</li>
                                     <li>Format: <code>Prompt: {&lt;visual_tokens&gt;, &lt;text prompt&gt;}</code>, <code>Response: {&lt;text response&gt;}</code></li>
                                 </ul>
                             </li>
                             <li><strong>Visual Generation Data:</strong>
                                 <ul>
                                     <li>Image-Text Pairs: Up to 5M pairs from the MetaCLIP pipeline, curated into instruction format.</li>
                                     <li>Format: <code>Prompt: {&lt;text prompt&gt;}</code>, <code>Response: {"Here is an image...", &lt;image_start&gt;, &lt;visual_tokens&gt;, &lt;image_end&gt;}</code></li>
                                 </ul>
                             </li>
                             <li><strong>Other Visual Data:</strong>
                                 <ul>
                                     <li>Video Data (HowTo100M, SomethingSomethingV2): Formatted for tasks like frame prediction, sequence completion, and temporal reasoning.</li>
                                     <li>Visual Thinking Data (Visual CoT, VStar): Includes visual generation within reasoning steps (e.g., generating zoomed views).</li>
                                     <li>Image-to-Image Data (InstructPix2Pix, Aurora): For conditioned image transformation tasks.</li>
                                 </ul>
                             </li>
                         </ul>
                     </div>
                 </div>
             </div>

             <div class="feature-card card" onclick="toggleVPiTFeature('vpit-feature3')">
                 <div class="feature-header">
                     <div class="feature-header-content">
                         <div class="feature-title">Visual Token Visualization</div>
                         <div class="feature-summary">A diffusion-based approach translates predicted visual tokens into images.</div>
                     </div>
                     <button class="toggle-button" id="toggle-vpit3">↓</button>
                 </div>
                 <div class="feature-details" id="vpit-feature3">
                     <div class="feature-visual">
                         <img src="static/img/diffusion.png" alt="Token Visualization Pipeline Diagram"
                              onerror="this.onerror=null; this.src='https://placehold.co/600x250/f8fafc/6b7280?text=Visualization+Pipeline';">
                     </div>
                     <div class="feature-content">
                         <p>To visualize the continuous visual tokens predicted by MetaMorph:</p>
                         <ul>
                             <li><strong>Training Stage (Diffusion Autoencoder):</strong>
                                 <ul>
                                     <li>We finetune an existing diffusion model (Stable Diffusion 1.5) to act as a visual decoder.</li>
                                     <li>It's trained to reconstruct images conditioned on the visual tokens produced by the <em>frozen</em> pretrained vision encoder (SigLIP).</li>
                                     <li>A simple MLP projector matches SigLIP embedding dimensions to the diffusion model's cross-attention dimension.</li>
                                     <li>Trained on held-out image-text data using standard latent diffusion objectives.</li>
                                 </ul>
                             </li>
                             <li><strong>Inference Pipeline:</strong>
                                 <ul>
                                     <li><strong>Step 1 (MetaMorph Prediction):</strong> The MetaMorph model processes the input prompt and generates a sequence of continuous visual tokens via its vision head.</li>
                                     <li><strong>Step 2 (Token Visualization):</strong> These predicted visual tokens are fed into the finetuned diffusion model (trained in the previous stage), conditioning the diffusion process to generate the final pixel-space image.</li>
                                 </ul>
                             </li>
                         </ul>
                         <p><strong>Note:</strong> This visualization step is primarily for analysis and demonstrating the model's capabilities, not for competing with state-of-the-art high-fidelity image generation models.</p>
                     </div>
                 </div>
             </div>
        </div>

        <script>
            // Simple toggle script for VPiT feature sections
            function toggleVPiTFeature(id) {
                const detailsElement = document.getElementById(id);
                const buttonElement = document.getElementById('toggle-' + id); // Assumes button ID follows pattern
                const isExpanded = detailsElement.classList.contains('expanded');

                detailsElement.classList.toggle('expanded');
                 if (buttonElement) {
                      buttonElement.style.transform = isExpanded ? 'rotate(0deg)' : 'rotate(180deg)';
                      buttonElement.innerHTML = isExpanded ? '↓' : '↑';
                 }
            }
        </script>
    </section>

    <section class="model-section section" id="metamorph">
        <div class="section-container">
            <div class="section-title">
                <h2>MetaMorph Model</h2>
                <p>A unified model demonstrating true multimodal capabilities through VPiT.</p>
            </div>

            <div class="feature-card card" onclick="toggleModelFeature('model-feature1')">
                <div class="feature-header">
                    <div class="feature-header-content">
                        <div class="feature-title">Competitive Performance</div>
                        <div class="feature-summary">Strong results across understanding and generation benchmarks using LLaMA-3.1 8B.</div>
                    </div>
                    <button class="toggle-button" id="toggle-model1">↓</button>
                </div>
                <div class="feature-details performance-details" id="model-feature1"> <div id="tab:final_table">
                         <div class="table-container">
                             <table class="data-table">
                                 <thead>
                                     <tr>
                                         <th colspan="2" class="tb-hdr">Model</th>
                                         <th colspan="9" class="tb-hdr">Image QA</th>
                                         <th colspan="1" class="tb-hdr">Video QA</th>
                                         <th colspan="1" class="tb-hdr">Generation</th>
                                     </tr>
                                     <tr>
                                         <th>Method</th>
                                         <th class="section-border">Base LLM</th>
                                         <th>MMBenchEN</th>
                                         <th>SEED</th>
                                         <th>RealWorldQA</th>
                                         <th>MMVP</th>
                                         <th>SQA</th>
                                         <th>MMMU</th>
                                         <th>VStar</th>
                                         <th>ChartQA</th>
                                         <th class="section-border">TextVQA</th>
                                         <th class="section-border">MV-Bench</th>
                                         <th class="section-border">FID ↓</th>
                                     </tr>
                                 </thead>
                                  <tbody>
                                     <tr>
                                         <td>GPT-4V*</td>
                                         <td class="section-border">-</td>
                                         <td>75.8</td> <td>69.1</td> <td>61.4</td> <td>50.0</td> <td>75.7</td> <td>56.8</td> <td>55.0</td> <td>78.5</td> <td class="section-border">78.0</td>
                                         <td class="section-border">43.5</td> <td class="section-border">-</td>
                                     </tr>
                                     <tr class="highlight-gray"><td colspan="13"><i>T2I Models</i></td></tr>
                                     <tr>
                                         <td>Stable Diffusion 1.5*</td>
                                         <td class="section-border">-</td>
                                         <td>-</td> <td>-</td> <td>-</td> <td>-</td> <td>-</td> <td>-</td> <td>-</td> <td>-</td> <td class="section-border">-</td>
                                         <td class="section-border">-</td> <td class="section-border">9.6</td>
                                     </tr>
                                     <tr>
                                         <td>Dalle 2*</td>
                                         <td class="section-border">-</td>
                                         <td>-</td> <td>-</td> <td>-</td> <td>-</td> <td>-</td> <td>-</td> <td>-</td> <td>-</td> <td class="section-border">-</td>
                                         <td class="section-border">-</td> <td class="section-border">10.4</td>
                                     </tr>
                                     <tr>
                                         <td>Imagen*</td>
                                         <td class="section-border">-</td>
                                         <td>-</td> <td>-</td> <td>-</td> <td>-</td> <td>-</td> <td>-</td> <td>-</td> <td>-</td> <td class="section-border">-</td>
                                         <td class="section-border">-</td> <td class="section-border">7.3</td>
                                     </tr>
                                     <tr class="highlight-gray"><td colspan="13"><i>Unified Models</i></td></tr>
                                     <tr>
                                         <td>EMU-3*</td>
                                         <td class="section-border">-</td>
                                         <td>58.5</td> <td>68.2</td> <td>57.4</td> <td>36.6†</td> <td>89.2</td> <td>31.6</td> <td>51.8†</td> <td>68.6</td> <td class="section-border">64.7</td>
                                         <td class="section-border">-</td> <td class="section-border">12.8</td>
                                     </tr>
                                     <tr>
                                         <td>Janus*</td>
                                         <td class="section-border">DeepSeek 1.3B</td>
                                         <td>69.4</td> <td>63.7</td> <td>-</td> <td>-</td> <td>-</td> <td>30.5</td> <td>-</td> <td>-</td> <td class="section-border">-</td>
                                         <td class="section-border">-</td> <td class="section-border">8.5</td>
                                     </tr>
                                     <tr>
                                         <td>VILA-U256†</td>
                                         <td class="section-border">LLaMA-2 7B</td>
                                         <td>66.6</td> <td>57.1</td> <td>46.6</td> <td>22.0</td> <td>67.1</td> <td>32.2</td> <td>38.7</td> <td>11.4</td> <td class="section-border">48.3*</td>
                                         <td class="section-border">40.8</td> <td class="section-border">19.6</td>
                                     </tr>
                                     <tr>
                                         <td>Transfusion*</td>
                                         <td class="section-border">-</td>
                                         <td>-</td> <td>-</td> <td>-</td> <td>-</td> <td>-</td> <td>-</td> <td>-</td> <td>-</td> <td class="section-border">-</td>
                                         <td class="section-border">-</td> <td class="section-border">6.7</td>
                                     </tr>
                                     <tr>
                                         <td>Chameleon-7B†</td>
                                         <td class="section-border">-</td>
                                         <td>35.7</td> <td>27.2</td> <td>19.6</td> <td>0.0</td> <td>50.3</td> <td>28.4</td> <td>37.1</td> <td>0.0</td> <td class="section-border">0.0</td>
                                         <td class="section-border">-</td> <td class="section-border">26.7*</td>
                                     </tr>
                                     <tr class="highlight-orange">
                                         <td><b>MetaMorph (Ours)</b></td>
                                         <td class="section-border"><b>LLaMA-3.1 8B</b></td>
                                         <td><b>75.2</b></td> <td><b>71.8</b></td> <td><b>58.3</b></td> <td><b>48.3</b></td> <td><b>83.2</b></td> <td><b>41.8</b></td> <td><b>44.0</b></td> <td><b>37.1</b></td> <td class="section-border"><b>60.5</b></td>
                                         <td class="section-border"><b>48.8</b></td> <td class="section-border"><b>11.8</b></td>
                                     </tr>
                                 </tbody>
                             </table>
                         </div>
                         <figcaption>
                             <strong>Table 1:</strong> Comparison with state-of-the-art models on various benchmarks. MetaMorph achieves competitive performance across both understanding and generation tasks. (*): Numbers reported in original papers. (†): Results reproduced using released weights. FID is measured on MS-COCO 2014 validation set (lower is better).
                         </figcaption>
                     </div>
                     <div class="performance-text">
                         <p>
                             MetaMorph demonstrates strong performance across a wide range of tasks, achieving results competitive with or superior to other open-source unified models of similar size. Notably, it performs well on both complex visual understanding benchmarks (like MMBench, SEED, MMMU) and shows reasonable visual generation quality (FID score), despite using only 64 tokens per image/frame and relying solely on instruction tuning without extensive pretraining stages common in other models.
                         </p>
                     </div>
                </div>
            </div>

            <div class="feature-card card" onclick="toggleModelFeature('model-feature2')">
                <div class="feature-header">
                    <div class="feature-header-content">
                        <div class="feature-title">LLM Knowledge Leverage</div>
                        <div class="feature-summary">Successfully utilizes pretrained LLM knowledge for visual generation.</div>
                    </div>
                    <button class="toggle-button" id="toggle-model2">↓</button>
                </div>
                <div class="feature-details" id="model-feature2">
                    <div class="feature-content">
                        <p>MetaMorph demonstrates the ability to leverage the knowledge and capabilities embedded within the pretrained language model for visual generation tasks:</p>
                        <ul>
                            <li>It can generate accurate visual representations for specialized concepts like "Chhogori" (K2 mountain), "Oncilla" (wild cat), and "Chizarira" (national park), tapping into the LLM's broad world knowledge.</li>
                            <li>The model exhibits nuanced semantic understanding, correctly visualizing prompts involving negation ("a glass without water") or complex relationships.</li>
                            <li>This transfer of knowledge allows MetaMorph to generate meaningful visuals for a wider range of concepts than models relying solely on paired image-text data.</li>
                        </ul>
                        <p>These examples suggest that unifying language and vision allows visual generation to be guided by the linguistic reasoning and background knowledge inherent in LLMs.</p>
                    </div>
                    <div class="visual-example">
                        <div class="examples-container">
                            <div class="example-nav">
                                <button onclick="event.stopPropagation(); previousExample()" class="nav-button">←</button>
                                <span id="example-counter">Example 1/5</span>
                                <button onclick="event.stopPropagation(); nextExample()" class="nav-button">→</button>
                            </div>
                            <div class="examples">
                                <div class="example active" data-index="0">
                                    <img src="static/img/chhogori.png" alt="Generated image of Chhogori (K2)"
                                         onerror="this.onerror=null; this.src='https://placehold.co/400x300/ffffff/6b7280?text=Chhogori';">
                                    <div class="example-text">
                                        <div class="prompt"><h4>Prompt:</h4><p>Generate an image of Chhogori</p></div>
                                        <div class="analysis"><h4>Explanation:</h4><p>Chhogori, also known as K2, is the second-highest mountain. The model uses its knowledge to generate the correct mountain.</p></div>
                                    </div>
                                </div>
                                <div class="example" data-index="1">
                                    <img src="static/img/oncilla.png" alt="Generated image of an Oncilla"
                                         onerror="this.onerror=null; this.src='https://placehold.co/400x300/ffffff/6b7280?text=Oncilla';">
                                    <div class="example-text">
                                        <div class="prompt"><h4>Prompt:</h4><p>Generate an image of an Oncilla</p></div>
                                        <div class="analysis"><h4>Explanation:</h4><p>The oncilla is a small spotted cat. The model accesses its knowledge base to visualize this specific animal.</p></div>
                                    </div>
                                </div>
                                <div class="example" data-index="2">
                                    <img src="static/img/chizarira.png" alt="Generated image of Chizarira National Park view"
                                         onerror="this.onerror=null; this.src='https://placehold.co/400x300/ffffff/6b7280?text=Chizarira';">
                                    <div class="example-text">
                                        <div class="prompt"><h4>Prompt:</h4><p>Generate an image of the view of Chizarira</p></div>
                                        <div class="analysis"><h4>Explanation:</h4><p>Chizarira is a national park in Zimbabwe. The model generates a plausible landscape view based on this geographic knowledge.</p></div>
                                    </div>
                                </div>
                                <div class="example" data-index="3">
                                     <img src="static/img/emptyglass.png" alt="Generated image of an empty glass"
                                          onerror="this.onerror=null; this.src='https://placehold.co/400x300/ffffff/6b7280?text=Empty+Glass';">
                                     <div class="example-text">
                                         <div class="prompt"><h4>Prompt:</h4><p>Generate an image of a glass without water</p></div>
                                         <div class="analysis"><h4>Explanation:</h4><p>MetaMorph correctly interprets the negation and generates an empty glass, demonstrating semantic understanding.</p></div>
                                     </div>
                                 </div>
                                 <div class="example" data-index="4">
                                     <img src="static/img/fullglass.png" alt="Generated image of a glass full of water"
                                          onerror="this.onerror=null; this.src='https://placehold.co/400x300/ffffff/6b7280?text=Full+Glass';">
                                     <div class="example-text">
                                         <div class="prompt"><h4>Prompt:</h4><p>Generate an image of a glass filled with water</p></div>
                                         <div class="analysis"><h4>Explanation:</h4><p>Contrasting the previous example, the model accurately generates a glass containing water when negation is removed.</p></div>
                                     </div>
                                 </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="feature-card card" onclick="toggleModelFeature('model-feature3')">
                 <div class="feature-header">
                     <div class="feature-header-content">
                         <div class="feature-title">Multimodal Reasoning Capabilities</div>
                         <div class="feature-summary">Demonstrates implicit reasoning within multimodal generation tasks.</div>
                     </div>
                     <button class="toggle-button" id="toggle-model3">↓</button>
                 </div>
                 <div class="feature-details" id="model-feature3">
                     <div class="feature-content">
                         <p>MetaMorph exhibits reasoning capabilities during generation, going beyond simple text-to-image mapping. Similar to how LLMs might precompute reasoning steps internally before generating text, MetaMorph appears to perform implicit reasoning before generating visual tokens:</p>
                         <ul>
                             <li>The model can decompose complex, multi-step prompts and implicitly solve them. For instance, given "Generate an image of the animal resulting from a monarch caterpillar's metamorphosis", MetaMorph internally reasons (caterpillar → metamorphosis → butterfly) and generates a butterfly image without explicit step-by-step instructions.</li>
                              <li>It handles prompts requiring world knowledge and logical deduction, like identifying the US flag from "Generate an image of the national flag of the country where Yellowstone National Park is located".</li>
                              <li>This implicit reasoning occurs without explicit chain-of-thought prompting, suggesting the model integrates reasoning directly into the generation process.</li>
                         </ul>
                         <p>These capabilities highlight the potential of unified models like MetaMorph to perform more complex, compositional tasks by leveraging the underlying LLM's reasoning abilities.</p>
                     </div>
                     <div class="visual-example">
                         <div class="examples-container">
                             <div class="example-nav">
                                 <button onclick="event.stopPropagation(); previousReasoningExample()" class="nav-button">←</button>
                                 <span id="reasoning-counter">Example 1/4</span>
                                 <button onclick="event.stopPropagation(); nextReasoningExample()" class="nav-button">→</button>
                             </div>
                             <div class="examples">
                                 <div class="reasoning-example active" data-index="0">
                                     <div class="prompt-box">"Generate an image of the animal resulting from a monarch caterpillar's metamorphosis"</div>
                                     <div class="result-image">
                                         <img src="static/img/butterfly.png" alt="Generated Monarch Butterfly"
                                              onerror="this.onerror=null; this.src='https://placehold.co/300x200/ffffff/6b7280?text=Butterfly';">
                                     </div>
                                     <div class="reasoning-process" onclick="event.stopPropagation()">
                                         <div class="process-header">Model's Implicit Reasoning Process</div>
                                         <div class="steps-progress">
                                             <div class="step-dot active" onclick="showStep(0, 0)">1</div><div class="step-line"></div>
                                             <div class="step-dot" onclick="showStep(0, 1)">2</div><div class="step-line"></div>
                                             <div class="step-dot" onclick="showStep(0, 2)">3</div>
                                         </div>
                                         <div class="steps-content">
                                             <div class="step-box" data-step="0"><div class="step-title">Input</div><div class="step-detail">Identifies starting subject: monarch caterpillar.</div></div>
                                             <div class="step-box hidden" data-step="1"><div class="step-title">Process</div><div class="step-detail">Applies knowledge of the 'metamorphosis' process.</div></div>
                                             <div class="step-box hidden" data-step="2"><div class="step-title">Output</div><div class="step-detail">Determines the result: monarch butterfly. Generates image.</div></div>
                                         </div>
                                     </div>
                                 </div>
                                 <div class="reasoning-example" data-index="1">
                                     <div class="prompt-box">"Generate an image of the national flag of the country where Yellowstone National Park is located"</div>
                                     <div class="result-image">
                                         <img src="static/img/usa.png" alt="Generated American Flag"
                                              onerror="this.onerror=null; this.src='https://placehold.co/300x200/ffffff/6b7280?text=US+Flag';">
                                     </div>
                                     <div class="reasoning-process" onclick="event.stopPropagation()">
                                         <div class="process-header">Model's Implicit Reasoning Process</div>
                                         <div class="steps-progress">
                                             <div class="step-dot active" onclick="showStep(1, 0)">1</div><div class="step-line"></div>
                                             <div class="step-dot" onclick="showStep(1, 1)">2</div><div class="step-line"></div>
                                             <div class="step-dot" onclick="showStep(1, 2)">3</div>
                                         </div>
                                         <div class="steps-content">
                                             <div class="step-box" data-step="0"><div class="step-title">Location ID</div><div class="step-detail">Identifies Yellowstone National Park.</div></div>
                                             <div class="step-box hidden" data-step="1"><div class="step-title">Country Link</div><div class="step-detail">Connects Yellowstone to the United States using geographic knowledge.</div></div>
                                             <div class="step-box hidden" data-step="2"><div class="step-title">Symbol Retrieval</div><div class="step-detail">Retrieves the national symbol (flag) for the USA. Generates image.</div></div>
                                         </div>
                                     </div>
                                 </div>
                                  <div class="reasoning-example" data-index="2">
                                     <div class="prompt-box">"Generate an image of the flower celebrated in spring festivals in the country where sushi originated"</div>
                                     <div class="result-image">
                                         <img src="static/img/cherryblossom.png" alt="Generated Cherry Blossoms"
                                              onerror="this.onerror=null; this.src='https://placehold.co/300x200/ffffff/6b7280?text=Cherry+Blossom';">
                                     </div>
                                     <div class="reasoning-process" onclick="event.stopPropagation()">
                                         <div class="process-header">Model's Implicit Reasoning Process</div>
                                         <div class="steps-progress">
                                             <div class="step-dot active" onclick="showStep(2, 0)">1</div><div class="step-line"></div>
                                             <div class="step-dot" onclick="showStep(2, 1)">2</div><div class="step-line"></div>
                                             <div class="step-dot" onclick="showStep(2, 2)">3</div>
                                         </div>
                                         <div class="steps-content">
                                             <div class="step-box" data-step="0"><div class="step-title">Origin ID</div><div class="step-detail">Identifies sushi origin: Japan.</div></div>
                                             <div class="step-box hidden" data-step="1"><div class="step-title">Cultural Link</div><div class="step-detail">Connects Japan, spring festivals, and celebrated flower: Cherry Blossom (Sakura).</div></div>
                                             <div class="step-box hidden" data-step="2"><div class="step-title">Image Generation</div><div class="step-detail">Generates image of cherry blossoms.</div></div>
                                         </div>
                                     </div>
                                 </div>
                                  <div class="reasoning-example" data-index="3">
                                     <div class="prompt-box">"Generate an image of the pet animal whose name is a rearrangement of the letters in the word 'tca'"</div>
                                     <div class="result-image">
                                         <img src="static/img/tca.png" alt="Generated Cat"
                                              onerror="this.onerror=null; this.src='https://placehold.co/300x200/ffffff/6b7280?text=Cat';">
                                     </div>
                                     <div class="reasoning-process" onclick="event.stopPropagation()">
                                         <div class="process-header">Model's Implicit Reasoning Process</div>
                                         <div class="steps-progress">
                                             <div class="step-dot active" onclick="showStep(3, 0)">1</div><div class="step-line"></div>
                                             <div class="step-dot" onclick="showStep(3, 1)">2</div><div class="step-line"></div>
                                             <div class="step-dot" onclick="showStep(3, 2)">3</div>
                                         </div>
                                         <div class="steps-content">
                                             <div class="step-box" data-step="0"><div class="step-title">Anagram</div><div class="step-detail">Rearranges 'tca' to 'cat'.</div></div>
                                             <div class="step-box hidden" data-step="1"><div class="step-title">Category ID</div><div class="step-detail">Identifies 'cat' as a common 'pet animal'.</div></div>
                                             <div class="step-box hidden" data-step="2"><div class="step-title">Image Generation</div><div class="step-detail">Generates image of a cat.</div></div>
                                         </div>
                                     </div>
                                 </div>
                             </div>
                         </div>
                     </div>
                 </div>
             </div>

        </div>

        <script>
            // Toggle for Model Features
            function toggleModelFeature(id) {
                const detailsElement = document.getElementById(id);
                const buttonElement = document.getElementById('toggle-' + id.split('-')[1] + id.split('-')[2]); // e.g., toggle-model1
                const isExpanded = detailsElement.classList.contains('expanded');

                detailsElement.classList.toggle('expanded');
                 if (buttonElement) {
                      buttonElement.style.transform = isExpanded ? 'rotate(0deg)' : 'rotate(180deg)';
                      buttonElement.innerHTML = isExpanded ? '↓' : '↑';
                 }
            }

            // Slider for Knowledge Examples
            let currentKnowledgeExample = 0;
            const totalKnowledgeExamples = 5; // Update if you add/remove examples
            const knowledgeExamples = document.querySelectorAll('#model-feature2 .example');
            const knowledgeCounter = document.getElementById('example-counter');

            function showKnowledgeExample(index) {
                knowledgeExamples.forEach((ex, i) => {
                    ex.classList.toggle('active', i === index);
                });
                if(knowledgeCounter) knowledgeCounter.textContent = `Example ${index + 1}/${totalKnowledgeExamples}`;
            }
            function nextExample(event) {
                 if(event) event.stopPropagation(); // Prevent card collapse
                 currentKnowledgeExample = (currentKnowledgeExample + 1) % totalKnowledgeExamples;
                 showKnowledgeExample(currentKnowledgeExample);
            }
            function previousExample(event) {
                 if(event) event.stopPropagation(); // Prevent card collapse
                 currentKnowledgeExample = (currentKnowledgeExample - 1 + totalKnowledgeExamples) % totalKnowledgeExamples;
                 showKnowledgeExample(currentKnowledgeExample);
            }
             // Initialize first knowledge example
             if (knowledgeExamples.length > 0) showKnowledgeExample(0);


            // Slider for Reasoning Examples
            let currentReasoningExample = 0;
            const totalReasoningExamples = 4; // Update if you add/remove examples
            const reasoningExamples = document.querySelectorAll('#model-feature3 .reasoning-example');
            const reasoningCounter = document.getElementById('reasoning-counter');

            function showReasoningExample(index) {
                 reasoningExamples.forEach((ex, i) => {
                     ex.classList.toggle('active', i === index);
                 });
                 if(reasoningCounter) reasoningCounter.textContent = `Example ${index + 1}/${totalReasoningExamples}`;
                 // Reset steps display for the new example
                 if(reasoningExamples[index]) {
                     showStep(index, 0); // Show first step of the new example
                 }
            }
            function nextReasoningExample(event) {
                 if(event) event.stopPropagation(); // Prevent card collapse
                 currentReasoningExample = (currentReasoningExample + 1) % totalReasoningExamples;
                 showReasoningExample(currentReasoningExample);
            }
            function previousReasoningExample(event) {
                 if(event) event.stopPropagation(); // Prevent card collapse
                 currentReasoningExample = (currentReasoningExample - 1 + totalReasoningExamples) % totalReasoningExamples;
                 showReasoningExample(currentReasoningExample);
            }

            // Function to show specific step within a reasoning example
            function showStep(exampleIndex, stepIndex) {
                 const activeExample = document.querySelector(`.reasoning-example[data-index="${exampleIndex}"]`);
                 if (!activeExample) return;

                 const steps = activeExample.querySelectorAll('.step-box');
                 const dots = activeExample.querySelectorAll('.step-dot');

                 steps.forEach(step => step.classList.add('hidden')); // Hide all steps first
                 dots.forEach(dot => dot.classList.remove('active')); // Deactivate all dots

                 const currentStepBox = activeExample.querySelector(`.step-box[data-step="${stepIndex}"]`);
                 if (currentStepBox) currentStepBox.classList.remove('hidden'); // Show target step

                 // Activate dots up to the current step
                 dots.forEach((dot, index) => {
                     if (index <= stepIndex) {
                         dot.classList.add('active');
                     }
                 });
            }
             // Initialize first reasoning example and its first step
             if (reasoningExamples.length > 0) showReasoningExample(0);

        </script>
    </section>

</body>
</html>
