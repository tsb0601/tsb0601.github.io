<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Peter Tong</title>

  <meta name="author" content="Jon Barron">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Peter Tong</name>
              </p>
              <p>Hi, I am Peter Tong, also go by the name Shengbang Tong(Á´•ÊôüÈÇ¶). I am a first-year PhD student in NYU Courant CS advised by Professor Yann LeCun and Professor Saining Xie. I recently graduateed from UC Berkeley with a triple major in Computer Science, Applied Mathematics(Honor) and Statistic(Honor). I am from Nanjing, China and Melbourne, Australia.
              </p>
              <p style="text-align:center">
                <a href="tsb@berkeley.edu">Email</a> &nbsp/&nbsp
                <a href="data/Peter_Tong_Resume.pdf">Resume</a> &nbsp/&nbsp
                <a href="https://twitter.com/TongPetersb">Twitter</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?hl=en&user=hYlbtl8AAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/tsb0601">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/PeterTong.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/PeterTong.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                 I recently graduated from UC Berkeley with a triple major. I am a first-year CS PhD student in NYU Courant advised by Prof. Yann LeCun and Prof. Saining Xie. I was a researcher in Berkeley Artificial Intelligence Lab(BAIR) advised by Prof. Yi Ma and Prof.Jacob Steinhardt. I am interested in world model, unsupervised/self-supervised learning, generative models and multimodal models. I would like to thank all my mentors-Yubei, Xili, Erik and collaborators for the incredible journey I had in my undergrad.
              </p>
            </td>
          </tr>
        </tbody></table>


      <!-- News section -->
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>News</heading>
            <ul>
              <li><strong>2024-01:</strong> Our paper <a href="https://arxiv.org/abs/2306.05272">Image Clustering in the Age of Pretrained Models</a> was accepted at ICLR 2024!</li>
              <li><strong>2023-12:</strong> Our papers  were accepted at CPAL 2024!</li>
              <li><strong>2023-09:</strong> I am helping organizing the <a href="https://gkioxari.github.io/Tutorials/iccv2023/">QVCV</a> workshop in ICCV. See you all in Paris!</li>
              <li><strong>2023-09:</strong> Our papers <a href="https://arxiv.org/abs/2306.12105">MultiMon</a> and <a href="https://arxiv.org/abs/2306.01129">CRATE(whitebox-transformer)</a> were accepted at NIPS 2023!</li>
              <li><strong>2023-07:</strong> Our paper <a href="https://arxiv.org/abs/2301.01805">Manifold Linearizing and Clustering</a> was accepted at ICCV 2023!</li>
              <li><strong>2023-05:</strong> I graduated from UC Berkeley with triple degree Applied Math (Honor), Statistics (Honor) and Computer Science (No honor, because I didn't want to take 16b and 61c too early, but I published quite some interesting work so yay)!!!</li>
              <li><strong>2023-04:</strong> I will be a CS PhD student in NYU Courant advised by <a href="https://yann.lecun.com/" target="_blank">Professor Yann LeCun</a> and <a href="https://sainingxie.com/" target="_blank">Professor Saining Xie</a>. Looking forward to working with Yann and Saining in New York!</li>
              <li><strong>2023-01:</strong> Our paper <a href="https://arxiv.org/abs/2202.05411">incremental-CTRL</a> was accepted at ICLR 2023!</li>
            </ul>
          </td>
        </tr>
      </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


          <heading>Publications & Preprints (* means equal contribution)</heading>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/MMVP.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2401.06209" id="MultiMon">
                <papertitle> Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs 
                </papertitle>
              </a>
              <br>
              <strong>Shengbang Tong</strong>, <a href = "https://liuzhuang13.github.io/">Zhuang Liu</a>, <a href = "https://yx-s-z.github.io/">Yuexiang Zhai</a>, <a href = "http://people.eecs.berkeley.edu/~yima/">Yi Ma</a>, <a href = "http://yann.lecun.com/">Yann LeCun</a>, <a href = "https://www.sainingxie.com/">Saining Xie</a>
              <br>
              <em>Under Review</em>
              <br>
              <p></p>
              <p>Is vision good enough for language? Our research reveals that the visual capabilities in recent multimodal LLMs (MLLMs) still exhibit systematic shortcomings. We identify ''CLIP-blind pairs'' - images that CLIP perceives as similar despite their clear visual differences. With these pairs, we construct the Multimodal Visual Patterns (MMVP) benchmark. MMVP exposes areas where state-of-the-art systems, including GPT-4V, struggle with straightforward questions across nine basic visual patterns. As an initial effort to address these issues, we propose a Mixture of Features (MoF) approach, demonstrating that integrating vision self-supervised learning features with MLLMs can significantly enhance their visual grounding capabilities.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/MultiMon.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2306.12105" id="MultiMon">
                <papertitle> Mass-Producing Failures of Multimodal Systems with Language Models </papertitle>
              </a>
              <br>
              <strong>Shengbang Tong*</strong>, <a href = "http://people.eecs.berkeley.edu/~erjones/">Erik Jones*</a>, <a href = "https://jsteinhardt.stat.berkeley.edu/">Jacob Steinhardt</a>
              <br>
              <em>Accepted by NIPS 2023</em>
              <br>
              <p></p>
              <p>Deployed multimodal systems can fail in ways that evaluators did not anticipate. In order to find these failures before deployment, we introduce MULTIMON, a system that automatically identifies systematic failures.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/t10ssl.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2304.03977" id="EMP-SSL">
                <papertitle> EMP-SSL: Towards Self-Supervised Learning in One Epoch </papertitle>
              </a>
              <br>
              <strong>Shengbang Tong*</strong>, <a href = "https://yubeichen.com/">Yubei Chen*</a>, <a href = "http://people.eecs.berkeley.edu/~yima/">Yi Ma</a>, <a href = "http://yann.lecun.com/">Yann LeCun</a>
              <br>
              <em>Under Review</em>
              <br>
              <p></p>
              <p>Inspired by the newly proposed principle, our work proposes a minimalist method for self-supervised learning that tremendously reduces the epochs which SSL methods take to converge.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/EMT.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2309.10313" id="EMT">
                <papertitle> Investigating the Catastrophic Forgetting in Multimodal Large Language Models </papertitle>
              </a>
              <br>
              <a href = "https://yx-s-z.github.io/">Yuexiang Zhai</a>, <strong>Shengbang Tong</strong>, <a href = "https://heimine.github.io/">Xiao Li</a>, <a href = "https://pages.cs.wisc.edu/~mucai/">Mu Cai</a>, <a href = "https://qingqu.engin.umich.edu/">Qing Qu</a>, <a href = "https://pages.cs.wisc.edu/~yongjaelee/">Yong Jae Lee</a>, <a href = "http://people.eecs.berkeley.edu/~yima/">Yi Ma</a>
              <br>
              <em>Accepted by CPAL 2024</em>
              <br>
              <p></p>
              <p>TLDR: Fine-Tuning multimodal large language models (MLLMs) leads to catastrophic forgetting.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/emergence.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2308.16271" id="emergence">
                <papertitle> Emergence of Segmentation with Minimalistic White-Box Transformers </papertitle>
              </a>
              <br>
              <a href = "https://yaodongyu.github.io/">Yaodong Yu*</a>, <a href = "https://tianzhechu.com/">Tianzhe Chu*</a>, <strong>Shengbang Tong</strong>, <a href = "https://robinwu218.github.io/">Ziyang Wu</a>, <a href = "https://druvpai.github.io/">Druv Pai</a>, <a href = "https://sdbuchanan.com/">Sam Buchanan</a>, <a href = "http://people.eecs.berkeley.edu/~yima/">Yi Ma</a>
              <br>
              <em>Accepted by CPAL 2024</em>
              <br>
              <p></p>
              <p>Through extensive experimental results, we demonstrate that when employing a white-box transformer-like architecture known as CRATE, whose design explicitly models and pursues low-dimensional structures in the data distribution, segmentation properties, at both the whole and parts levels, already emerge with a minimalistic supervised training recipe. Layer-wise finer-grained analysis reveals that the emergent properties strongly corroborate the designed mathematical functions of the white-box network.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/CPP.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2306.05272" id="CPP">
                <papertitle> Image Clustering via the Principle of Rate Reduction in the Age of Pretrained Models </papertitle>
              </a>
              <br>
              <a href = "https://tianzhechu.com/">Tianzhe Chu*</a>, <strong>Shengbang Tong*</strong>, <a href = "https://tianjiaoding.com/">Tianjiao Ding*</a>, <a href = "https://delay-xili.github.io/">Xili Dai</a>, <a href = "https://www.cis.jhu.edu/~haeffele/">Benjamin Haeffele</a>, <a href = "http://vision.jhu.edu/rvidal.html">Rene Vidal</a>, <a href = "http://people.eecs.berkeley.edu/~yima/">Yi Ma</a>
              <br>
              <em>Accepted by ICLR 2024</em>
              <br>
              <p></p>
              <p>In this paper, we propose a novel image clustering pipeline that leverages the powerful feature representation of large pre-trained models such as CLIP and cluster images effectively and efficiently at scale. We show that the pre-trained features are significantly more structured by further optimizing the rate reduction objective.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/CRATE.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2306.01129" id="CRATE">
                <papertitle> White-Box Transformers via Sparse Rate Reduction </papertitle>
              </a>
              <br>
              <a href = "https://yaodongyu.github.io/">Yaodong Yu</a>, <a href = "https://sdbuchanan.com/">Sam Buchanan</a>, <a href = "https://druvpai.github.io/">Druv Pai</a>, <a href = "https://tianzhechu.com/">Tianzhe Chu</a>, <a href = "https://robinwu218.github.io/">Ziyang Wu</a>, <strong>Shengbang Tong</strong>, <a href = "https://www.cis.jhu.edu/~haeffele/">Benjamin Haeffele</a>, <a href = "http://people.eecs.berkeley.edu/~yima/">Yi Ma</a>
              <br>
              <em>Accepted by NIPS 2023</em>
              <br>
              <p></p>
              <p>In this paper, we contend that the objective of representation learning is to compress and transform the distribution of the data towards a mixture of low-dimensional Gaussian distributions supported on incoherent subspaces. From this perspective, popular deep networks such as transformers can be naturally viewed as realizing iterative schemes to optimize this objective incrementally. </p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/uCTRL.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2210.16782" id="uctrl">
                <papertitle>Unsupervised Learning of Structured Representation via Closed-Loop Transcription</papertitle>
              </a>
              <br>
              <strong>Shengbang Tong*</strong>, <a href = "https://delay-xili.github.io/">Xili Dai*</a>, <a href = "https://yubeichen.com/">Yubei Chen</a>, <a>Mingyang Li</a>, <a>Zengyi Li</a>,  <a>Brent Yi</a>, <a href = "http://yann.lecun.com/">Yann LeCun</a>, <a href = "http://people.eecs.berkeley.edu/~yima/">Yi Ma</a>
              <br>
              <em>Accepted by CPAL 2024</em>
              <br>
              <p></p>
              <p>This paper proposes a new unsupervised method to learn a structured representation that may serve both discriminative and generative purpose</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/CSC_CTRL.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2302.09347" id="umlc">
                <papertitle> Closed-Loop Transcription Via Convolutional Sparse Coding</papertitle>
              </a>
              <br>
              <a>Xili Dai</a>, <a>Ke Chen</a>, <strong>Shengbang Tong</strong>, <a>Jingyuan Zhang</a>, <a>Xingjian Gao</a>, <a>Mingyang Li</a>, <a>Druv Pai</a>, <a>Yuexiang Zhai</a>, <a>Xiaojun Yuan</a>, <a>Heung Yeung Shum</a>, <a>Lionel M.Ni</a>, <a href = "http://people.eecs.berkeley.edu/~yima/">Yi Ma</a>
              <br>
              <em>Accepted by CPAL 2024</em>
              <br>
              <p></p>
              <p>This paper explores the natural inverse in Covolutional Sparse Coding neural network and its application in generative models.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/MCRSE.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2301.01805" id="umlc">
                <papertitle>Unsupervised Manifold Linearizing and Clustering</papertitle>
              </a>
              <br>
              <a href = "https://tianjiaoding.com/">Tianjiao Ding</a>, <strong>Shengbang Tong</strong>, <a>Kwan Ho Ryan Chan</a>, <a href = "https://delay-xili.github.io/">Xili Dai</a>, <a href = "http://people.eecs.berkeley.edu/~yima/">Yi Ma</a>,<a>Benjamin David Haeffele</a>
              <br>
              <em>Accepted by ICCV 2023</em>
              <br>
              <p></p>
              <p>This paper proposes a new unsupervised method to learn a represenation and cluster for real life dataset such as CIFAR-10, CIFAR100 and Tiny-ImageNet-200.</p>
            </td>
          </tr>




          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/sdnet.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2210.12945" id="revisit">
                <papertitle>Revisiting Sparse Convolutional Model for Visual Recognition</papertitle>
              </a>
              <br>
              <a href = "https://delay-xili.github.io/">Xili Dai*</a>,  <a>Mingyang Li*</a>, <a>Pengyuan Zhai</a>,  <strong>Shengbang Tong</strong>, <a>Xingjian Gao</a>, <a>Shaolun Huang</a>, <a>Zhihui Zhu</a>, <a>Chong You</a>, <a href = "http://people.eecs.berkeley.edu/~yima/">Yi Ma</a>
              <br>
              <em>Accepted by NIPS 2022</em>
              <br>
              <p></p>
              <p>Our method uses differentiable optimization layers that are defined from convolutional sparse coding as drop-in replacements of standard convolutional layers in conventional deep neural networks. We show that such models have equally strong empirical performance on CIFAR-10, CIFAR-100 and ImageNet datasets when compared to conventional neural networks.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/iLDR.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2202.05411" id="iLDR">
                <papertitle>Incremental Learning of Structured Memory via Closed-Loop Transcription</papertitle>
              </a>
              <br>
              <strong>Shengbang Tong</strong>, <a href = "https://delay-xili.github.io/">Xili Dai</a>, <a>Ziyang Wu</a>, <a>Mingyang Li</a>, <a>Brent Yi</a>, <a href = "http://people.eecs.berkeley.edu/~yima/">Yi Ma</a>
              <br>
              <em>Accepted by ICLR 2023</em>
              <br>
              <p></p>
              <p>We propose a minimal computational model for learning a structured memory of multiple object classes in an incremental setting</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/LDR.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2111.06636" id="LDR">
                <papertitle>Closed-Loop Data Transcription to an LDR via Minimaxing Rate Reduction</papertitle>
              </a>
              <br>
              <a href = "https://delay-xili.github.io/">Xili Dai*</a>, <strong>Shengbang Tong*</strong>, <a>Mingyang Li*</a>, <a>Ziyang Wu*</a>, <a>Kwan Ho Ryan Chan</a>, <a>Pengyuan Zhai</a>, <a>Yaodong Yu</a>, <a>Michael Psenka</a>, <a>Xiaojun Yuan</a>, <a>Heung Yeung Shum</a>, <a href = "http://people.eecs.berkeley.edu/~yima/">Yi Ma</a>
              <br>
              <em>Accepted by Entropy Journal</em>
              <br>
              <p></p>
              <p>We propose a new computational framework for learning an explicit generative model for real-world dataset.</p>
            </td>
          </tr>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Mentorship</heading>
              <p>
                Feel free to reach out if you are interested in computer vision and want to chat with me. 
              </p>
            </td>
          </tr>
        </tbody></table>

        <style>
          .sub-table {
              width: 0;
              height: 0;
              overflow: hidden;
              position: absolute;
              left: -9999px;
              opacity: 0;
              visibility: hidden;
          }
      </style>
      
      <table class="sub-table" align="center">
          <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=y35-AqSkeLIkce_C13W-97DGULFZQWj5YJB3rNARabY&cl=ffffff&w=a"></script>
      </table>
      
        </tbody></table>




      </td>
    </tr>
  </table>
</body>

</html>
