<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Peter Tong</title>

  <meta name="author" content="Jon Barron">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Peter Tong</name>
              </p>
              <p>Hi, I am Peter Tong, also go by the name Shengbang Tong(Á´•ÊôüÈÇ¶). I am a second-year PhD student in NYU Courant CS advised by Professor Yann LeCun and Professor Saining Xie. I recently graduateed from UC Berkeley with a triple major in Computer Science, Applied Mathematics(Honor) and Statistic(Honor). I am from Nanjing, China and Melbourne, Australia.  
              </p>
              <p style="text-align:center">
                <a href="tsb@berkeley.edu">Email</a> &nbsp/&nbsp
                <a href="data/Peter_Tong_Resume.pdf">Resume</a> &nbsp/&nbsp
                <a href="https://twitter.com/TongPetersb">Twitter</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?hl=en&user=hYlbtl8AAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/tsb0601">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/PeterTong.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/PeterTong.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                 I graduated from UC Berkeley with a triple major. I am a second-year CS PhD student in NYU Courant advised by Prof. Yann LeCun and Prof. Saining Xie. I was a researcher in Berkeley Artificial Intelligence Lab(BAIR) advised by Prof. Yi Ma and Prof.Jacob Steinhardt. I am interested in world model, unsupervised/self-supervised learning, generative models and multimodal models. I would like to thank all my mentors-Yubei, Xili, Erik and collaborators for the incredible journey I had in my undergrad.
              </p>
            </td>
          </tr>
        </tbody></table>


      <!-- News section -->
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>News</heading>
            <ul>
              <li><strong>2024-09:</strong> Our paper <a href="https://arxiv.org/abs/2405.10292">RLVLM</a> was accepted at NeurIPS2024, and <a href="https://arxiv.org/abs/2406.16860">Cambrian</a> was accepted at NeurIPS 2024 as an Oral Paper!</li>
              <li><strong>2024-05:</strong> I joined FAIR, Meta for summer internship with Dr.  <a href = "https://liuzhuang13.github.io/">Zhuang Liu</a>,  yayyyyy!</li>
              <li><strong>2024-04:</strong> I received <a href="https://openai.com/blog/superalignment-fast-grants">OpenAI Superalignment Fellowship</a>! Thank you OpenAI!!! Looking forward to the cool works.</li>
              <li><strong>2024-04:</strong> Our paper <a href="https://arxiv.org/abs/2401.06209">Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs</a> was accepted at CVPR 2024 as an Oral Paper!</li>
              <li><strong>2024-01:</strong> Our paper <a href="https://arxiv.org/abs/2306.05272">Image Clustering in the Age of Pretrained Models</a> was accepted at ICLR 2024!</li>
              <li><strong>2023-12:</strong> Our papers  were accepted at CPAL 2024!</li>
              <li><strong>2023-09:</strong> I am helping organizing the <a href="https://gkioxari.github.io/Tutorials/iccv2023/">QVCV</a> workshop in ICCV. See you all in Paris!</li>
              <li><strong>2023-09:</strong> Our papers <a href="https://arxiv.org/abs/2306.12105">MultiMon</a> and <a href="https://arxiv.org/abs/2306.01129">CRATE(whitebox-transformer)</a> were accepted at NeurIPS 2023!</li>
              <li><strong>2023-07:</strong> Our paper <a href="https://arxiv.org/abs/2301.01805">Manifold Linearizing and Clustering</a> was accepted at ICCV 2023!</li>
              <li><strong>2023-05:</strong> I graduated from UC Berkeley with triple degree Applied Math (Honor), Statistics (Honor) and Computer Science (No honor, because I didn't want to take 16b and 61c too early, but I published quite some interesting work so yay)!!!</li>
              <li><strong>2023-04:</strong> I will be a CS PhD student in NYU Courant advised by <a href="https://yann.lecun.com/" target="_blank">Professor Yann LeCun</a> and <a href="https://sainingxie.com/" target="_blank">Professor Saining Xie</a>. Looking forward to working with Yann and Saining in New York!</li>
              <li><strong>2023-01:</strong> Our paper <a href="https://arxiv.org/abs/2202.05411">incremental-CTRL</a> was accepted at ICLR 2023!</li>
            </ul>
          </td>
        </tr>
      </tbody></table>
        

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


          <heading>Publications & Preprints (* means equal contribution)</heading>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/metamorph.gif" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2412.14164v1" id="MultiMon">
                <papertitle> MetaMorph: Multimodal Understanding and Generation via Instruction Tuning
                </papertitle>
              </a>
              <br>
              <strong>Shengbang Tong</strong>,
              <a href="https://davidfan.io/" target="_blank">David Fan</a>,
              <a href="https://jiachenzhu.github.io/" target="_blank">Jiachen Zhu</a>,
              <a href="https://pages.cs.wisc.edu/~yunyang/" target="_blank">Yunyang Xiong</a>,
              <a href="https://xinleic.xyz/" target="_blank">Xinlei Chen</a>,
              <a href="https://koustuvsinha.com/" target="_blank">Koustuv Sinha</a>,
              <a href="https://ai.meta.com/people/1148536089838617/michael-rabbat/" target="_blank">Michael Rabbat</a>,
              <a href="https://yann.lecun.com/" target="_blank">Yann LeCun</a>,
              <a href="https://www.sainingxie.com/" target="_blank">Saining Xie</a>,
              <a href="https://liuzhuang13.github.io/" target="_blank">Zhuang Liu</a>
              <br>
              <em>Under Review</span>

              <br>
              <p></p>
              <p>Visual understanding and visual generation are mutually benefitial in unified models! But visual understanding data is much more effective than visual generation. Capabilities in LLM can also transfer to unified models such as implicit reasoning! </p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/cambrian.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2406.16860" id="MultiMon">
                <papertitle> Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs
                </papertitle>
              </a>
              <br>
              <strong>Shengbang Tong*</strong>,
              <a href="https://ellisbrown.github.io/" target="_blank">Ellis Brown*</a>,
              <a href="https://penghao-wu.github.io/" target="_blank">Penghao Wu*</a>,
              <a href="https://sites.google.com/view/sanghyunwoo/" target="_blank">Sanghyun Woo</a>,
              <a href="https://www.linkedin.com/in/manoj-middepogu/" target="_blank">Manoj Middepogu</a>,
              <a href="https://www.linkedin.com/in/sai-charitha-akula-32574887/" target="_blank">Sai Charitha Akula</a>,
              <a href="https://jihanyang.github.io/" target="_blank">Jihan Yang</a>,
              <a href="https://github.com/vealocia" target="_blank">Shusheng Yang</a>,
              <a href="https://adithyaiyer1999.github.io/" target="_blank">Adithya Iyer</a>,
              <a href="https://xichenpan.com/" target="_blank">Xichen Pan</a>,
              <a href="https://www.linkedin.com/in/ziteng-wang-694b8b227/" target="_blank">Austin Wang</a>,
              <a href="http://cs.nyu.edu/~fergus" target="_blank">Rob Fergus</a>,
              <a href="http://yann.lecun.com/" target="_blank">Yann LeCun</a>,
              <a href="https://www.sainingxie.com/" target="_blank">Saining Xie</a>
  
              <br>
              <em>NeurIPS 2024 </em><span style="color: red;"><strong>Oral</strong></span>

              <br>
              <p></p>
              <p>We provide a vision-centric exploration or cookbook in MLLMs. In other words, we systematically study visual representation, vision-language connector, instruction tuning data, training recipe and evaluation protocols in MLLMs. We propose new vision-centric benchmarks, spatial-aware connector, data collection and curation of instruciton data, and more! We also release very competitive 8B, 13B and 34B models on par with GPT-4V and Gemini. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/MMVP.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2401.06209" id="MultiMon">
                <papertitle> Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs 
                </papertitle>
              </a>
              <br>
              <strong>Shengbang Tong</strong>, <a href = "https://liuzhuang13.github.io/">Zhuang Liu</a>, <a href = "https://yx-s-z.github.io/">Yuexiang Zhai</a>, <a href = "http://people.eecs.berkeley.edu/~yima/">Yi Ma</a>, <a href = "http://yann.lecun.com/">Yann LeCun</a>, <a href = "https://www.sainingxie.com/">Saining Xie</a>
              <br>
              <em>CVPR 2024 </em><span style="color: red;"><strong>Oral</strong></span>




              <br>
              <p></p>
              <p>Is vision good enough for language? Our research reveals that the visual capabilities in recent multimodal LLMs (MLLMs) still exhibit systematic shortcomings. We identify ''CLIP-blind pairs'' - images that CLIP perceives as similar despite their clear visual differences. With these pairs, we construct the Multimodal Visual Patterns (MMVP) benchmark. MMVP exposes areas where state-of-the-art systems, including GPT-4V, struggle with straightforward questions across nine basic visual patterns. As an initial effort to address these issues, we propose a Mixture of Features (MoF) approach, demonstrating that integrating vision self-supervised learning features with MLLMs can significantly enhance their visual grounding capabilities.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/rlmllm.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2405.10292" id="MultiMon">
                <papertitle> Fine-Tuning Large Vision-Language Models as Decision-Making Agents via Reinforcement Learning
                </papertitle>
              </a>
              <br>
              <a href = "https://yx-s-z.github.io/">Yuexiang Zhai</a>, <a href = "https://biechi.github.io/">Hao Bai*</a>, <a>Zipeng Lin*</a>, <a href = "https://www.jiayipan.me/">Jiayi Pan*</a>, <strong>Shengbang Tong*</strong>, <a href = "https://yifeizhou02.github.io/">Yifei Zhou*</a>, <a href="https://www.alanesuhr.com/">Alen Suhr</a>, <a href = "https://www.sainingxie.com/">Saining Xie</a>, <a href = "http://yann.lecun.com/">Yann LeCun</a>, <a href = "http://people.eecs.berkeley.edu/~yima/">Yi Ma</a>, <a href = "https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>
              <br>
              <em> NeurIPS 2024</em>




              <br>
              <p></p>
              <p>We can use RL to train MLLM instead on SFT! Using RL to train from environment feedback unlocks model's ability in decision making, exceeding the limitations in SFT. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/MultiMon.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2306.12105" id="MultiMon">
                <papertitle> Mass-Producing Failures of Multimodal Systems with Language Models </papertitle>
              </a>
              <br>
              <strong>Shengbang Tong*</strong>, <a href = "http://people.eecs.berkeley.edu/~erjones/">Erik Jones*</a>, <a href = "https://jsteinhardt.stat.berkeley.edu/">Jacob Steinhardt</a>
              <br>
              <em> NeurIPS 2023</em>
              <br>
              <p></p>
              <p>Deployed multimodal systems can fail in ways that evaluators did not anticipate. In order to find these failures before deployment, we introduce MULTIMON, a system that automatically identifies systematic failures.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/t10ssl.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2304.03977" id="EMP-SSL">
                <papertitle> EMP-SSL: Towards Self-Supervised Learning in One Epoch </papertitle>
              </a>
              <br>
              <strong>Shengbang Tong*</strong>, <a href = "https://yubeichen.com/">Yubei Chen*</a>, <a href = "http://people.eecs.berkeley.edu/~yima/">Yi Ma</a>, <a href = "http://yann.lecun.com/">Yann LeCun</a>
              <br>
              <em>Under Review</em>
              <br>
              <p></p>
              <p>Inspired by the newly proposed principle, our work proposes a minimalist method for self-supervised learning that tremendously reduces the epochs which SSL methods take to converge.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/EMT.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2309.10313" id="EMT">
                <papertitle> Investigating the Catastrophic Forgetting in Multimodal Large Language Models </papertitle>
              </a>
              <br>
              <a href = "https://yx-s-z.github.io/">Yuexiang Zhai</a>, <strong>Shengbang Tong</strong>, <a href = "https://heimine.github.io/">Xiao Li</a>, <a href = "https://pages.cs.wisc.edu/~mucai/">Mu Cai</a>, <a href = "https://qingqu.engin.umich.edu/">Qing Qu</a>, <a href = "https://pages.cs.wisc.edu/~yongjaelee/">Yong Jae Lee</a>, <a href = "http://people.eecs.berkeley.edu/~yima/">Yi Ma</a>
              <br>
              <em> CPAL 2024</em>
              <br>
              <p></p>
              <p>TLDR: Fine-Tuning multimodal large language models (MLLMs) leads to catastrophic forgetting.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/emergence.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2308.16271" id="emergence">
                <papertitle> Emergence of Segmentation with Minimalistic White-Box Transformers </papertitle>
              </a>
              <br>
              <a href = "https://yaodongyu.github.io/">Yaodong Yu*</a>, <a href = "https://tianzhechu.com/">Tianzhe Chu*</a>, <strong>Shengbang Tong</strong>, <a href = "https://robinwu218.github.io/">Ziyang Wu</a>, <a href = "https://druvpai.github.io/">Druv Pai</a>, <a href = "https://sdbuchanan.com/">Sam Buchanan</a>, <a href = "http://people.eecs.berkeley.edu/~yima/">Yi Ma</a>
              <br>
              <em> CPAL 2024</em>
              <br>
              <p></p>
              <p>Through extensive experimental results, we demonstrate that when employing a white-box transformer-like architecture known as CRATE, whose design explicitly models and pursues low-dimensional structures in the data distribution, segmentation properties, at both the whole and parts levels, already emerge with a minimalistic supervised training recipe. Layer-wise finer-grained analysis reveals that the emergent properties strongly corroborate the designed mathematical functions of the white-box network.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/CPP.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2306.05272" id="CPP">
                <papertitle> Image Clustering via the Principle of Rate Reduction in the Age of Pretrained Models </papertitle>
              </a>
              <br>
              <a href = "https://tianzhechu.com/">Tianzhe Chu*</a>, <strong>Shengbang Tong*</strong>, <a href = "https://tianjiaoding.com/">Tianjiao Ding*</a>, <a href = "https://delay-xili.github.io/">Xili Dai</a>, <a href = "https://www.cis.jhu.edu/~haeffele/">Benjamin Haeffele</a>, <a href = "http://vision.jhu.edu/rvidal.html">Rene Vidal</a>, <a href = "http://people.eecs.berkeley.edu/~yima/">Yi Ma</a>
              <br>
              <em> ICLR 2024</em>
              <br>
              <p></p>
              <p>In this paper, we propose a novel image clustering pipeline that leverages the powerful feature representation of large pre-trained models such as CLIP and cluster images effectively and efficiently at scale. We show that the pre-trained features are significantly more structured by further optimizing the rate reduction objective.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/CRATE.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2306.01129" id="CRATE">
                <papertitle> White-Box Transformers via Sparse Rate Reduction </papertitle>
              </a>
              <br>
              <a href = "https://yaodongyu.github.io/">Yaodong Yu</a>, <a href = "https://sdbuchanan.com/">Sam Buchanan</a>, <a href = "https://druvpai.github.io/">Druv Pai</a>, <a href = "https://tianzhechu.com/">Tianzhe Chu</a>, <a href = "https://robinwu218.github.io/">Ziyang Wu</a>, <strong>Shengbang Tong</strong>, <a href = "https://www.cis.jhu.edu/~haeffele/">Benjamin Haeffele</a>, <a href = "http://people.eecs.berkeley.edu/~yima/">Yi Ma</a>
              <br>
              <em> NeurIPS 2023</em>
              <br>
              <p></p>
              <p>In this paper, we contend that the objective of representation learning is to compress and transform the distribution of the data towards a mixture of low-dimensional Gaussian distributions supported on incoherent subspaces. From this perspective, popular deep networks such as transformers can be naturally viewed as realizing iterative schemes to optimize this objective incrementally. </p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/uCTRL.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2210.16782" id="uctrl">
                <papertitle>Unsupervised Learning of Structured Representation via Closed-Loop Transcription</papertitle>
              </a>
              <br>
              <strong>Shengbang Tong*</strong>, <a href = "https://delay-xili.github.io/">Xili Dai*</a>, <a href = "https://yubeichen.com/">Yubei Chen</a>, <a>Mingyang Li</a>, <a>Zengyi Li</a>,  <a>Brent Yi</a>, <a href = "http://yann.lecun.com/">Yann LeCun</a>, <a href = "http://people.eecs.berkeley.edu/~yima/">Yi Ma</a>
              <br>
              <em> CPAL 2024</em>
              <br>
              <p></p>
              <p>This paper proposes a new unsupervised method to learn a structured representation that may serve both discriminative and generative purpose</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/CSC_CTRL.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2302.09347" id="umlc">
                <papertitle> Closed-Loop Transcription Via Convolutional Sparse Coding</papertitle>
              </a>
              <br>
              <a>Xili Dai</a>, <a>Ke Chen</a>, <strong>Shengbang Tong</strong>, <a>Jingyuan Zhang</a>, <a>Xingjian Gao</a>, <a>Mingyang Li</a>, <a>Druv Pai</a>, <a>Yuexiang Zhai</a>, <a>Xiaojun Yuan</a>, <a>Heung Yeung Shum</a>, <a>Lionel M.Ni</a>, <a href = "http://people.eecs.berkeley.edu/~yima/">Yi Ma</a>
              <br>
              <em> CPAL 2024</em>
              <br>
              <p></p>
              <p>This paper explores the natural inverse in Covolutional Sparse Coding neural network and its application in generative models.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/MCRSE.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2301.01805" id="umlc">
                <papertitle>Unsupervised Manifold Linearizing and Clustering</papertitle>
              </a>
              <br>
              <a href = "https://tianjiaoding.com/">Tianjiao Ding</a>, <strong>Shengbang Tong</strong>, <a>Kwan Ho Ryan Chan</a>, <a href = "https://delay-xili.github.io/">Xili Dai</a>, <a href = "http://people.eecs.berkeley.edu/~yima/">Yi Ma</a>,<a>Benjamin David Haeffele</a>
              <br>
              <em> ICCV 2023</em>
              <br>
              <p></p>
              <p>This paper proposes a new unsupervised method to learn a represenation and cluster for real life dataset such as CIFAR-10, CIFAR100 and Tiny-ImageNet-200.</p>
            </td>
          </tr>




          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/sdnet.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2210.12945" id="revisit">
                <papertitle>Revisiting Sparse Convolutional Model for Visual Recognition</papertitle>
              </a>
              <br>
              <a href = "https://delay-xili.github.io/">Xili Dai*</a>,  <a>Mingyang Li*</a>, <a>Pengyuan Zhai</a>,  <strong>Shengbang Tong</strong>, <a>Xingjian Gao</a>, <a>Shaolun Huang</a>, <a>Zhihui Zhu</a>, <a>Chong You</a>, <a href = "http://people.eecs.berkeley.edu/~yima/">Yi Ma</a>
              <br>
              <em> NeurIPS 2022</em>
              <br>
              <p></p>
              <p>Our method uses differentiable optimization layers that are defined from convolutional sparse coding as drop-in replacements of standard convolutional layers in conventional deep neural networks. We show that such models have equally strong empirical performance on CIFAR-10, CIFAR-100 and ImageNet datasets when compared to conventional neural networks.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/iLDR.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2202.05411" id="iLDR">
                <papertitle>Incremental Learning of Structured Memory via Closed-Loop Transcription</papertitle>
              </a>
              <br>
              <strong>Shengbang Tong</strong>, <a href = "https://delay-xili.github.io/">Xili Dai</a>, <a>Ziyang Wu</a>, <a>Mingyang Li</a>, <a>Brent Yi</a>, <a href = "http://people.eecs.berkeley.edu/~yima/">Yi Ma</a>
              <br>
              <em> ICLR 2023</em>
              <br>
              <p></p>
              <p>We propose a minimal computational model for learning a structured memory of multiple object classes in an incremental setting</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/LDR.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2111.06636" id="LDR">
                <papertitle>Closed-Loop Data Transcription to an LDR via Minimaxing Rate Reduction</papertitle>
              </a>
              <br>
              <a href = "https://delay-xili.github.io/">Xili Dai*</a>, <strong>Shengbang Tong*</strong>, <a>Mingyang Li*</a>, <a>Ziyang Wu*</a>, <a>Kwan Ho Ryan Chan</a>, <a>Pengyuan Zhai</a>, <a>Yaodong Yu</a>, <a>Michael Psenka</a>, <a>Xiaojun Yuan</a>, <a>Heung Yeung Shum</a>, <a href = "http://people.eecs.berkeley.edu/~yima/">Yi Ma</a>
              <br>
              <em> Entropy Journal</em>
              <br>
              <p></p>
              <p>We propose a new computational framework for learning an explicit generative model for real-world dataset.</p>
            </td>
          </tr>
          
          

        <style>
          .sub-table {
              width: 0;
              height: 0;
              overflow: hidden;
              position: absolute;
              left: -9999px;
              opacity: 0;
              visibility: hidden;
          }
      </style>
      
      <table class="sub-table" align="center">
          <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=y35-AqSkeLIkce_C13W-97DGULFZQWj5YJB3rNARabY&cl=ffffff&w=a"></script>
      </table>
      
        </tbody></table>




      </td>
    </tr>
  </table>
</body>

</html>
