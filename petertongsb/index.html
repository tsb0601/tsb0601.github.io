<!DOCTYPE HTML>
<html lang="en">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Peter Tong</title>
  <meta name="author" content="Peter Tong">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
  <style>
    /* Base styles */
    body {
      font-family: sans-serif;
      line-height: 1.5;
      color: #333;
      margin: 0;
      padding: 0;
      background-color: #fff;
    }
    
    .container {
      max-width: 800px;
      margin: 0 auto;
      padding: 20px;
    }
    
    /* Header */
    .header {
      display: flex;
      align-items: center;
      margin-bottom: 30px;
    }
    
    .profile-content {
      flex: 3;
      padding-right: 20px;
    }
    
    .profile-image {
      flex: 1;
      text-align: center;
    }
    
    .profile-image img {
      max-width: 100%;
    }
    
    .name {
      font-size: 2.2rem;
      color: #333;
      margin-bottom: 15px;
      text-align: center;
    }
    
    .bio {
      margin-bottom: 20px;
      font-size: 1rem;
    }
    
    .contact-links {
      display: flex;
      justify-content: center;
      flex-wrap: wrap;
      gap: 15px;
      margin-top: 20px;
    }
    
    .contact-links a {
      text-decoration: none;
      color: #0000EE;
    }
    
    .contact-links a:hover {
      text-decoration: underline;
    }
    
    /* Sections */
    .section {
      margin-bottom: 30px;
      padding-bottom: 15px;
    }
    
    .section-title {
      font-size: 1.5rem;
      color: #333;
      margin-bottom: 15px;
      padding-bottom: 5px;
      border-bottom: 1px solid #ddd;
    }
    
    /* News */
    .news-list {
      list-style-type: none;
      padding-left: 0;
    }
    
    .news-item {
      margin-bottom: 12px;
    }
    
    .news-date {
      font-weight: bold;
    }
    
    /* Publications */
    .publication-grid {
      display: grid;
      grid-template-columns: 1fr;
      gap: 25px;
    }
    
    .publication {
      display: flex;
      background-color: #fff;
      border-bottom: 1px solid #eee;
      padding-bottom: 20px;
    }
    
    .pub-image {
      width: 160px;
      flex-shrink: 0;
      margin-right: 15px;
    }
    
    .pub-image img {
      width: 100%;
      height: auto;
    }
    
    .pub-content {
      flex-grow: 1;
    }
    
    .pub-title {
      font-size: 1.1rem;
      margin-bottom: 8px;
      color: #0000EE;
    }
    
    .pub-authors {
      font-size: 0.95rem;
      margin-bottom: 8px;
    }
    
    .pub-venue {
      font-style: italic;
      margin-bottom: 8px;
      font-size: 0.95rem;
    }
    
    .pub-description {
      font-size: 0.95rem;
      color: #333;
    }
    
    /* All Publications List Style */
    .all-publications-list {
      list-style-type: none;
      padding-left: 0;
    }
    
    .pub-item {
      margin-bottom: 20px;
      padding-bottom: 20px;
      border-bottom: 1px solid #eee;
    }
    
    .pub-item:last-child {
      border-bottom: none;
    }
    
    .pub-item-title {
      font-size: 1.1rem;
      margin-bottom: 6px;
    }
    
    .pub-item-title a {
      color: #0000EE;
      text-decoration: none;
    }
    
    .pub-item-title a:hover {
      text-decoration: underline;
    }
    
    .pub-item-authors {
      font-size: 0.9rem;
      margin-bottom: 5px;
    }
    
    .pub-item-venue {
      font-style: italic;
      font-size: 0.9rem;
      margin-bottom: 5px;
    }
    
    .highlight {
      color: #cc0000;
      font-weight: bold;
    }
    
    /* Publication Tabs */
    .pub-tabs {
      display: flex;
      border-bottom: 1px solid #ddd;
      margin-bottom: 25px;
    }
    
    .pub-tab {
      padding: 8px 15px;
      cursor: pointer;
      border: 1px solid transparent;
      border-bottom: none;
      background-color: transparent;
      font-weight: normal;
      color: #333;
    }
    
    .pub-tab.active {
      color: #0000EE;
      background-color: #fff;
      border-color: #ddd;
      border-bottom-color: #fff;
      margin-bottom: -1px;
    }
    
    .pub-tab:hover:not(.active) {
      color: #0000EE;
    }
    
    .pub-tab-content {
      display: none;
    }
    
    .pub-tab-content.active {
      display: block;
    }
    
    /* Responsive */
    @media (max-width: 768px) {
      .header {
        flex-direction: column-reverse;
      }
      
      .profile-content {
        padding-right: 0;
        margin-top: 20px;
      }
      
      .profile-image {
        margin-bottom: 20px;
      }
      
      .publication {
        flex-direction: column;
      }
      
      .pub-image {
        width: 100%;
        margin-bottom: 15px;
      }
    }
  </style>
  <script>
    function switchTab(tabName) {
      // Hide all tab contents
      document.querySelectorAll('.pub-tab-content').forEach(function(tabContent) {
        tabContent.classList.remove('active');
      });
      
      // Deactivate all tabs
      document.querySelectorAll('.pub-tab').forEach(function(tab) {
        tab.classList.remove('active');
      });
      
      // Activate selected tab and content
      if (tabName === 'selected') {
        document.getElementById('selected-tab').classList.add('active');
        document.querySelectorAll('.pub-tab')[0].classList.add('active');
      } else if (tabName === 'all') {
        document.getElementById('all-tab').classList.add('active');
        document.querySelectorAll('.pub-tab')[1].classList.add('active');
      }
    }
  </script>
</head>

<body>
  <div class="container">
    
    <!-- Header Section -->
    <div class="header" id="about">
      <div class="profile-content">
        <h1 class="name">Peter Tong</h1>
        <p class="bio">
          Hi, I am Peter Tong, also go by the name Shengbang Tong(Á´•ÊôüÈÇ¶). I am a second-year PhD student in NYU Courant CS advised by Professor Yann LeCun and Professor Saining Xie. I recently graduated from UC Berkeley with a triple major in Computer Science, Applied Mathematics(Honor) and Statistics(Honor). I am from Nanjing, China and Melbourne, Australia.
        </p>
        <div class="contact-links">
          <a href="mailto:tsb@berkeley.edu">Email</a>
          <a href="data/Peter_Tong_Resume.pdf">Resume</a>
          <a href="https://twitter.com/TongPetersb">Twitter</a>
          <a href="https://scholar.google.com/citations?hl=en&user=hYlbtl8AAAAJ">Google Scholar</a>
          <a href="https://github.com/tsb0601">Github</a>
        </div>
      </div>
      <div class="profile-image">
        <img src="images/PeterTong.jpg" alt="Peter Tong">
      </div>
    </div>
    
    <!-- Research Section -->
    <div class="section" id="research">
      <h2 class="section-title">Research</h2>
      <p>
        I graduated from UC Berkeley with a triple major. I am a second-year CS PhD student in NYU Courant advised by Prof. Yann LeCun and Prof. Saining Xie. I was a researcher in Berkeley Artificial Intelligence Lab(BAIR) advised by Prof. Yi Ma and Prof. Jacob Steinhardt. I am interested in world model, unsupervised/self-supervised learning, generative models and multimodal models. I would like to thank all my mentors-Yubei, Xili, Erik and collaborators for the incredible journey I had in my undergrad.
      </p>
    </div>
    
    <!-- News Section -->
    <div class="section" id="news">
      <h2 class="section-title">News</h2>
      <ul class="news-list">
        <li class="news-item">
          <span class="news-date">2024-09:</span> Our paper <a href="https://arxiv.org/abs/2405.10292">RLVLM</a> was accepted at NeurIPS2024, and <a href="https://arxiv.org/abs/2406.16860">Cambrian</a> was accepted at NeurIPS 2024 as an Oral Paper!
        </li>
        <li class="news-item">
          <span class="news-date">2024-05:</span> I joined FAIR, Meta for summer internship with Dr. <a href="https://liuzhuang13.github.io/">Zhuang Liu</a>, yayyyyy!
        </li>
        <li class="news-item">
          <span class="news-date">2024-04:</span> I received <a href="https://openai.com/blog/superalignment-fast-grants">OpenAI Superalignment Fellowship</a>! Thank you OpenAI!!! Looking forward to the cool works.
        </li>
        <li class="news-item">
          <span class="news-date">2024-04:</span> Our paper <a href="https://arxiv.org/abs/2401.06209">Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs</a> was accepted at CVPR 2024 as an Oral Paper!
        </li>
        <li class="news-item">
          <span class="news-date">2024-01:</span> Our paper <a href="https://arxiv.org/abs/2306.05272">Image Clustering in the Age of Pretrained Models</a> was accepted at ICLR 2024!
        </li>
        <li class="news-item">
          <span class="news-date">2023-12:</span> Our papers were accepted at CPAL 2024!
        </li>
        <li class="news-item">
          <span class="news-date">2023-09:</span> I am helping organizing the <a href="https://gkioxari.github.io/Tutorials/iccv2023/">QVCV</a> workshop in ICCV. See you all in Paris!
        </li>
        <li class="news-item">
          <span class="news-date">2023-09:</span> Our papers <a href="https://arxiv.org/abs/2306.12105">MultiMon</a> and <a href="https://arxiv.org/abs/2306.01129">CRATE(whitebox-transformer)</a> were accepted at NeurIPS 2023!
        </li>
        <li class="news-item">
          <span class="news-date">2023-07:</span> Our paper <a href="https://arxiv.org/abs/2301.01805">Manifold Linearizing and Clustering</a> was accepted at ICCV 2023!
        </li>
        <li class="news-item">
          <span class="news-date">2023-05:</span> I graduated from UC Berkeley with triple degree Applied Math (Honor), Statistics (Honor) and Computer Science (No honor, because I didn't want to take 16b and 61c too early, but I published quite some interesting work so yay)!!!
        </li>
        <li class="news-item">
          <span class="news-date">2023-04:</span> I will be a CS PhD student in NYU Courant advised by <a href="https://yann.lecun.com/" target="_blank">Professor Yann LeCun</a> and <a href="https://sainingxie.com/" target="_blank">Professor Saining Xie</a>. Looking forward to working with Yann and Saining in New York!
        </li>
        <li class="news-item">
          <span class="news-date">2023-01:</span> Our paper <a href="https://arxiv.org/abs/2202.05411">incremental-CTRL</a> was accepted at ICLR 2023!
        </li>
      </ul>
    </div>
    
    <!-- Publications Section with Tabs -->
    <div class="section" id="publications">
      <h2 class="section-title">Publications</h2>
      
      <!-- Publication Tabs -->
      <div class="pub-tabs">
        <button class="pub-tab active" onclick="switchTab('selected')">Selected Publications</button>
        <button class="pub-tab" onclick="switchTab('all')">All Publications</button>
      </div>
      
      <!-- Selected Publications Tab Content -->
      <div id="selected-tab" class="pub-tab-content active">
        <div class="publication-grid">
          <!-- Publication 3: MetaMorph -->
          <div class="publication">
            <div class="pub-image">
              <img src="images/metamorph.gif" alt="MetaMorph">
            </div>
            <div class="pub-content">
              <h3 class="pub-title">
                <a href="https://arxiv.org/abs/2412.14164v1">MetaMorph: Multimodal Understanding and Generation via Instruction Tuning</a>
              </h3>
              <div class="pub-authors">
                <strong>Shengbang Tong</strong>, <a href="https://davidfan.io/" target="_blank">David Fan</a>, <a href="https://jiachenzhu.github.io/" target="_blank">Jiachen Zhu</a>, <a href="https://pages.cs.wisc.edu/~yunyang/" target="_blank">Yunyang Xiong</a>, <a href="https://xinleic.xyz/" target="_blank">Xinlei Chen</a>, <a href="https://koustuvsinha.com/" target="_blank">Koustuv Sinha</a>, <a href="https://ai.meta.com/people/1148536089838617/michael-rabbat/" target="_blank">Michael Rabbat</a>, <a href="https://yann.lecun.com/" target="_blank">Yann LeCun</a>, <a href="https://www.sainingxie.com/" target="_blank">Saining Xie</a>, <a href="https://liuzhuang13.github.io/" target="_blank">Zhuang Liu</a>
              </div>
              <div class="pub-venue">Technical Report</div>
              <p class="pub-description">
                Visual understanding and visual generation are mutually beneficial in unified models! But visual understanding data is much more effective than visual generation. Capabilities in LLM can also transfer to unified models such as implicit reasoning!
              </p>
            </div>
          </div>


          <!-- Publication 1: Cambrian-1 -->
          <div class="publication">
            <div class="pub-image">
              <img src="images/cambrian.png" alt="Cambrian">
            </div>
            <div class="pub-content">
              <h3 class="pub-title">
                <a href="https://arxiv.org/abs/2406.16860">Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs</a>
              </h3>
              <div class="pub-authors">
                <strong>Shengbang Tong*</strong>, <a href="https://ellisbrown.github.io/" target="_blank">Ellis Brown*</a>, <a href="https://penghao-wu.github.io/" target="_blank">Penghao Wu*</a>, <a href="https://sites.google.com/view/sanghyunwoo/" target="_blank">Sanghyun Woo</a>, <a href="https://www.linkedin.com/in/manoj-middepogu/" target="_blank">Manoj Middepogu</a>, <a href="https://www.linkedin.com/in/sai-charitha-akula-32574887/" target="_blank">Sai Charitha Akula</a>, <a href="https://jihanyang.github.io/" target="_blank">Jihan Yang</a>, <a href="https://github.com/vealocia" target="_blank">Shusheng Yang</a>, <a href="https://adithyaiyer1999.github.io/" target="_blank">Adithya Iyer</a>, <a href="https://xichenpan.com/" target="_blank">Xichen Pan</a>, <a href="https://www.linkedin.com/in/ziteng-wang-694b8b227/" target="_blank">Austin Wang</a>, <a href="http://cs.nyu.edu/~fergus" target="_blank">Rob Fergus</a>, <a href="http://yann.lecun.com/" target="_blank">Yann LeCun</a>, <a href="https://www.sainingxie.com/" target="_blank">Saining Xie</a>
              </div>
              <div class="pub-venue">NeurIPS 2024 <span class="highlight">Oral</span></div>
              <p class="pub-description">
                We provide a vision-centric exploration or cookbook in MLLMs, systematically studying visual representation, vision-language connector, instruction tuning data, training recipe and evaluation protocols. We propose new vision-centric benchmarks, spatial-aware connector, data collection and curation of instruction data, and release very competitive 8B, 13B and 34B models on par with GPT-4V and Gemini.
              </p>
            </div>
          </div>
          
          <!-- Publication 2: Eyes Wide Shut -->
          <div class="publication">
            <div class="pub-image">
              <img src="images/MMVP.png" alt="MMVP">
            </div>
            <div class="pub-content">
              <h3 class="pub-title">
                <a href="https://arxiv.org/abs/2401.06209">Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs</a>
              </h3>
              <div class="pub-authors">
                <strong>Shengbang Tong</strong>, <a href="https://liuzhuang13.github.io/" target="_blank">Zhuang Liu</a>, <a href="https://yx-s-z.github.io/" target="_blank">Yuexiang Zhai</a>, <a href="http://people.eecs.berkeley.edu/~yima/" target="_blank">Yi Ma</a>, <a href="http://yann.lecun.com/" target="_blank">Yann LeCun</a>, <a href="https://www.sainingxie.com/" target="_blank">Saining Xie</a>
              </div>
              <div class="pub-venue">CVPR 2024 <span class="highlight">Oral</span></div>
              <p class="pub-description">
                Is vision good enough for language? Our research reveals that the visual capabilities in recent multimodal LLMs (MLLMs) still exhibit systematic shortcomings. We identify 'CLIP-blind pairs' - images that CLIP perceives as similar despite their clear visual differences. With these pairs, we construct the Multimodal Visual Patterns (MMVP) benchmark.
              </p>
            </div>
          </div>


           <!-- Publication 4: MultiMon -->
           <div class="publication">
            <div class="pub-image">
              <img src="images/MultiMon.png" alt="MultiMon">
            </div>
            <div class="pub-content">
              <h3 class="pub-title">
                <a href="https://arxiv.org/abs/2306.12105">Mass-Producing Failures of Multimodal Systems with Language Models</a>
              </h3>
              <div class="pub-authors">
                <strong>Shengbang Tong*</strong>, <a href="http://people.eecs.berkeley.edu/~erjones/" target="_blank">Erik Jones*</a>, <a href="https://jsteinhardt.stat.berkeley.edu/" target="_blank">Jacob Steinhardt</a>
              </div>
              <div class="pub-venue">NeurIPS 2023</div>
              <p class="pub-description">
                Deployed multimodal systems can fail in ways that evaluators did not anticipate. In order to find these failures before deployment, we introduce MULTIMON, a system that automatically identifies systematic failures.
              </p>
            </div>
          </div>
          

        </div>
      </div>
      
      <!-- All Publications Tab Content -->
      <div id="all-tab" class="pub-tab-content">
        <ul class="all-publications-list">

                    <!-- New Publications -->
          <li class="pub-item">
            <h3 class="pub-item-title">
              <a href="https://arxiv.org/abs/2501.17161">SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training</a>
            </h3>
            <div class="pub-item-authors">
              Tianzhe Chu*, Yuexiang Zhai*, Jihan Yang, <strong>Shengbang Tong</strong>, Saining Xie, Dale Schuurmans, Quoc V. Le, Sergey Levine, Yi Ma
            </div>
            <div class="pub-item-venue">Technical Report</div>
          </li>
          

          <li class="pub-item">
            <h3 class="pub-item-title">
              <a href="https://arxiv.org/abs/2412.14164v1">MetaMorph: Multimodal Understanding and Generation via Instruction Tuning</a>
            </h3>
            <div class="pub-item-authors">
              <strong>Shengbang Tong</strong>, David Fan, Jiachen Zhu, Yunyang Xiong, Xinlei Chen, Koustuv Sinha, Michael Rabbat, Yann LeCun, Saining Xie, Zhuang Liu
            </div>
            <div class="pub-item-venue">Technical Report</div>
          </li>

          <!-- 2024 Publications -->
          <li class="pub-item">
            <h3 class="pub-item-title">
              <a href="https://arxiv.org/abs/2406.16860">Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs</a>
            </h3>
            <div class="pub-item-authors">
              <strong>Shengbang Tong*</strong>, Ellis Brown*, Penghao Wu*, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, Austin Wang, Rob Fergus, Yann LeCun, Saining Xie
            </div>
            <div class="pub-item-venue">NeurIPS 2024 <span class="highlight">Oral</span></div>
          </li>
          
          <li class="pub-item">
            <h3 class="pub-item-title">
              <a href="https://arxiv.org/abs/2405.10292">Fine-Tuning Large Vision-Language Models as Decision-Making Agents via Reinforcement Learning</a>
            </h3>
            <div class="pub-item-authors">
              Yuexiang Zhai, Hao Bai*, Zipeng Lin*, Jiayi Pan*, <strong>Shengbang Tong*</strong>, Yifei Zhou*, Alen Suhr, Saining Xie, Yann LeCun, Yi Ma, Sergey Levine
            </div>
            <div class="pub-item-venue">NeurIPS 2024</div>
          </li>

          
          <li class="pub-item">
            <h3 class="pub-item-title">
              <a href="https://proceedings.neurips.cc/paper_files/paper/2024/hash/04a80267ad46fc730011f8760f265054-Abstract-Conference.html">Connecting Joint-Embedding Predictive Architecture with Contrastive Self-supervised Learning</a>
            </h3>
            <div class="pub-item-authors">
              Shentong Mo*, <strong>Shengbang Tong*</strong>
            </div>
            <div class="pub-item-venue">NeurIPS 2024 <span class="highlight">Spotlight</span></div>
          </li>
          
          

          
          <li class="pub-item">
            <h3 class="pub-item-title">
              <a href="https://arxiv.org/abs/2401.06209">Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs</a>
            </h3>
            <div class="pub-item-authors">
              <strong>Shengbang Tong</strong>, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, Saining Xie
            </div>
            <div class="pub-item-venue">CVPR 2024 <span class="highlight">Oral</span></div>
          </li>
          
          <li class="pub-item">
            <h3 class="pub-item-title">
              <a href="https://arxiv.org/abs/2309.10313">Investigating the Catastrophic Forgetting in Multimodal Large Language Models</a>
            </h3>
            <div class="pub-item-authors">
              Yuexiang Zhai, <strong>Shengbang Tong</strong>, Xiao Li, Mu Cai, Qing Qu, Yong Jae Lee, Yi Ma
            </div>
            <div class="pub-item-venue">CPAL 2024</div>
          </li>

          <li class="pub-item">
            <h3 class="pub-item-title">
              <a href="https://arxiv.org/abs/2308.16271">Emergence of Segmentation with Minimalistic White-Box Transformers</a>
            </h3>
            <div class="pub-item-authors">
              Yaodong Yu*, Tianzhe Chu*, <strong>Shengbang Tong</strong>, Ziyang Wu, Druv Pai, Sam Buchanan, Yi Ma
            </div>
            <div class="pub-item-venue">CPAL 2024</div>
          </li>

          <li class="pub-item">
            <h3 class="pub-item-title">
              <a href="https://arxiv.org/abs/2403.10953">Ctrl123: Consistent Novel View Synthesis via Closed-Loop Transcription</a>
            </h3>
            <div class="pub-item-authors">
              Hongxiang Zhao*, Xili Dai*, Jianan Wang, <strong>Shengbang Tong</strong>, Jingyuan Zhang, Weida Wang, Lei Zhang, Yi Ma
            </div>
            <div class="pub-item-venue">Technical Report</div>
          </li>

          
          <!-- 2023 Publications -->
          <li class="pub-item">
            <h3 class="pub-item-title">
              <a href="https://arxiv.org/abs/2306.12105">Mass-Producing Failures of Multimodal Systems with Language Models</a>
            </h3>
            <div class="pub-item-authors">
              <strong>Shengbang Tong*</strong>, Erik Jones*, Jacob Steinhardt
            </div>
            <div class="pub-item-venue">NeurIPS 2023</div>
          </li>

          <li class="pub-item">
            <h3 class="pub-item-title">
              <a href="https://arxiv.org/abs/2306.05272">Image Clustering in the Age of Pretrained Models</a>
            </h3>
            <div class="pub-item-authors">
              Tianzhe Chu*, <strong>Shengbang Tong*</strong>, Tianjiao Ding*, Xili Dai, Benjamin Haeffele, Rene Vidal, Yi Ma
            </div>
            <div class="pub-item-venue">ICLR 2024</div>
          </li>

          <li class="pub-item">
            <h3 class="pub-item-title">
              <a href="https://arxiv.org/abs/2306.01129">White-Box Transformers via Sparse Rate Reduction</a>
            </h3>
            <div class="pub-item-authors">
              Yaodong Yu, Sam Buchanan, Druv Pai, Tianzhe Chu, Ziyang Wu, <strong>Shengbang Tong</strong>, Benjamin Haeffele, Yi Ma
            </div>
            <div class="pub-item-venue">NeurIPS 2023</div>
          </li>

          <li class="pub-item">
            <h3 class="pub-item-title">
              <a href="https://arxiv.org/abs/2304.03977">EMP-SSL: Towards Self-Supervised Learning in One Epoch</a>
            </h3>
            <div class="pub-item-authors">
              <strong>Shengbang Tong*</strong>, Yubei Chen*, Yi Ma, Yann LeCun
            </div>
            <div class="pub-item-venue">Technical Report</div>
          </li>

          <li class="pub-item">
            <h3 class="pub-item-title">
              <a href="https://arxiv.org/abs/2301.01805">Unsupervised Manifold Linearizing and Clustering</a>
            </h3>
            <div class="pub-item-authors">
              Tianjiao Ding, <strong>Shengbang Tong</strong>, Kwan Ho Ryan Chan, Xili Dai, Yi Ma, Benjamin David Haeffele
            </div>
            <div class="pub-item-venue">ICCV 2023</div>
          </li>

          <li class="pub-item">
            <h3 class="pub-item-title">
              <a href="https://arxiv.org/abs/2302.09347">Closed-Loop Transcription Via Convolutional Sparse Coding</a>
            </h3>
            <div class="pub-item-authors">
              Xili Dai, Ke Chen, <strong>Shengbang Tong</strong>, Jingyuan Zhang, Xingjian Gao, Mingyang Li, Druv Pai, Yuexiang Zhai, Xiaojun Yuan, Heung Yeung Shum, Lionel M.Ni, Yi Ma
            </div>
            <div class="pub-item-venue">CPAL 2024</div>
          </li>
          
          <li class="pub-item">
            <h3 class="pub-item-title">
              <a href="https://arxiv.org/abs/2210.16782">Unsupervised Learning of Structured Representation via Closed-Loop Transcription</a>
            </h3>
            <div class="pub-item-authors">
              <strong>Shengbang Tong*</strong>, Xili Dai*, Yubei Chen, Mingyang Li, Zengyi Li, Brent Yi, Yann LeCun, Yi Ma
            </div>
            <div class="pub-item-venue">CPAL 2024</div>
          </li>

           <!-- 2022 Publications -->
           <li class="pub-item">
            <h3 class="pub-item-title">
              <a href="https://arxiv.org/abs/2210.12945">Revisiting Sparse Convolutional Model for Visual Recognition</a>
            </h3>
            <div class="pub-item-authors">
              Xili Dai*, Mingyang Li*, Pengyuan Zhai, <strong>Shengbang Tong</strong>, Xingjian Gao, Shaolun Huang, Zhihui Zhu, Chong You, Yi Ma
            </div>
            <div class="pub-item-venue">NeurIPS 2022</div>
          </li>

          <li class="pub-item">
            <h3 class="pub-item-title">
              <a href="https://arxiv.org/abs/2202.05411">Incremental Learning of Structured Memory via Closed-Loop Transcription</a>
            </h3>
            <div class="pub-item-authors">
              <strong>Shengbang Tong</strong>, Xili Dai, Ziyang Wu, Mingyang Li, Brent Yi, Yi Ma
            </div>
            <div class="pub-item-venue">ICLR 2023</div>
          </li>
          
          <li class="pub-item">
            <h3 class="pub-item-title">
              <a href="https://arxiv.org/abs/2111.06636">Closed-Loop Data Transcription to an LDR via Minimaxing Rate Reduction</a>
            </h3>
            <div class="pub-item-authors">
              Xili Dai*, <strong>Shengbang Tong*</strong>, Mingyang Li*, Ziyang Wu*, Kwan Ho Ryan Chan, Pengyuan Zhai, Yaodong Yu, Michael Psenka, Xiaojun Yuan, Heung Yeung Shum, Yi Ma
            </div>
            <div class="pub-item-venue">Entropy Journal</div>
          </li>
          
         
        </ul>
      </div>
    </div>
    
    <!-- Footer -->
    <div style="text-align: center; margin-top: 50px; color: #777; font-size: 0.9rem;">
      ¬© 2025 Peter Tong. Last updated: March 2025.
    </div>
  </div>
</body>
</html>